{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb145ce-b820-4ca8-ad4e-09b4e6a993e7",
   "metadata": {},
   "source": [
    "# Adaboost implementation\n",
    "\n",
    "Pseudocode\n",
    "1) Initialization- a weight for each sample data point (same in the beginning)\n",
    "2) Create a decision stump which maximizes information gain\n",
    "3) Quantify the power of say for that decision stump\n",
    "4) Calculate the new weights for each sample data point by using the power of say and erroneous predictions\n",
    "5) Use new weights to go back to step 2 and create a decision stump by utilizing the weights of each data point when calculating the information gain as well as the power of say\n",
    "6) After n iterations, when n different decision stumps are created, utilize the prediction power of each stump and reach an output.\n",
    "    Starting from the first stump, multiple the power of say of that stump with its prediction output (binary class)\n",
    "7) The final decision is based on the sign of the output of step 6 i.e if the sum is positive then its the positive class else negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281a9b45-365f-4362-b81d-c3f07c35b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data sampling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import make_classification\n",
    "epsilon= 1e-10\n",
    "\n",
    "# creating a decision stump\n",
    "\n",
    "def calculate_impurity(node):\n",
    "    # calculating probability of each class for impurity calculation\n",
    "    weight_adjusted_values_pos= node[node['target']==0]['weights'].sum()\n",
    "    weight_adjusted_values_neg= node[node['target']==1]['weights'].sum()\n",
    "    # total weight in the node\n",
    "    total_weight = node['weights'].sum()\n",
    "    total_weight = max(total_weight, epsilon) \n",
    "    # calculating probability of each class based on weights\n",
    "    probability_of_pos_class = weight_adjusted_values_pos / total_weight\n",
    "    probability_of_neg_class = weight_adjusted_values_neg / total_weight\n",
    "    \n",
    "\n",
    "    # calculating the positive and negative entropy\n",
    "    positive= -1 * ( probability_of_pos_class*math.log(probability_of_pos_class+ epsilon))\n",
    "    negative= -1 * ( probability_of_neg_class*math.log(probability_of_neg_class+epsilon))\n",
    "    return positive+negative\n",
    "\n",
    "def stump_for_feature(dataframe, x):\n",
    "    # calculate the impurity at the root node (before any split)\n",
    "    impurity_at_root = calculate_impurity(dataframe)\n",
    "\n",
    "    # get sorted values of feature x to create possible thresholds\n",
    "    possible_values = sorted(set(dataframe[x]))\n",
    "    # instead of taking actual points, we calculate midpoints between consecutive unique values for potential thresholds\n",
    "    possible_thresholds = [(possible_values[i] + possible_values[i + 1]) / 2 for i in range(len(possible_values) - 1)]\n",
    "\n",
    "    max_gain = -1 * math.inf\n",
    "    max_threshold = None\n",
    "\n",
    "    # Iterate over all possible thresholds to find out which value gives the most gain in information\n",
    "    for threshold in possible_thresholds:\n",
    "        # Split the data based on the threshold\n",
    "        left_node = dataframe[dataframe[x] >= threshold]\n",
    "        right_node = dataframe[dataframe[x] < threshold]\n",
    "\n",
    "        # impurity for both resulting nodes\n",
    "        a = calculate_impurity(left_node)\n",
    "        b = calculate_impurity(right_node)\n",
    "\n",
    "        # weighted impurity\n",
    "        weighted_impurity = (left_node['weights'].sum() / dataframe['weights'].sum()) * a + (right_node['weights'].sum() / dataframe['weights'].sum()) * b\n",
    "\n",
    "\n",
    "        information_gain = impurity_at_root - weighted_impurity\n",
    "\n",
    "        # Check for max gain\n",
    "        if information_gain > max_gain:\n",
    "            max_gain = information_gain\n",
    "            max_threshold = threshold\n",
    "\n",
    "    return max_gain, max_threshold\n",
    "\n",
    "\n",
    "def get_best_stump(data):\n",
    "    stump_dict= {}\n",
    "    for i in range(len(data.columns)):\n",
    "        if data.columns[i]!=\"target\":\n",
    "            gain, threshold = stump_for_feature(data, data.columns[i])\n",
    "            stump_dict[data.columns[i]]= (gain,threshold)\n",
    "    max_feature, (max_gain, max_threshold) = max(stump_dict.items(), key=lambda item: item[1][0])\n",
    "    return max_feature, max_gain, max_threshold\n",
    "\n",
    "\n",
    "def calculate_error(data, feature, threshold):\n",
    "    # misclassifications\n",
    "    error=0\n",
    "    indexes=[]\n",
    "    data['new_target']= data[feature].apply(lambda x: 1 if x<= threshold else 0)\n",
    "    \n",
    "    data['classification_correctness'] = data.apply(lambda row: 0 if row['target'] != row['new_target'] else 1, axis=1)\n",
    "    weighted_error = data.apply(lambda row: row['weights'] if row['target'] != row['new_target'] else 0, axis=1).sum()\n",
    "    return weighted_error / data['weights'].sum(), data\n",
    "\n",
    "    # return data['classification_correctness'].sum()/len(data), data\n",
    "\n",
    "def calculate_amount_of_say(total_error):\n",
    "    amount_of_say= 0.5*math.log((1/total_error+epsilon)-1)\n",
    "    return amount_of_say\n",
    "\n",
    "def weight_update(old_weight, say, sign):\n",
    "    new_weight = old_weight * math.exp(sign * say)\n",
    "    # print(f\"Old weight: {old_weight}, Say: {say}, Sign: {sign}, New weight: {new_weight}\")\n",
    "    return new_weight\n",
    "\n",
    "def reweight(power_of_say, data):\n",
    "    data['weights']= data.apply(lambda x: weight_update(x['weights'], power_of_say, -1) if x['classification_correctness']==0 else weight_update(x['weights'], power_of_say, 1), axis=1)\n",
    "    data['weights'] = data['weights'] / data['weights'].sum()\n",
    "    return data\n",
    "\n",
    "# Final prediction for each sample in the dataset\n",
    "def predict(data, individual_learners):\n",
    "    final_predictions = [0] * len(data)\n",
    "    for feature, threshold, alpha in individual_learners:\n",
    "        # Make predictions for current stump\n",
    "        predictions = data[feature].apply(lambda x: 1 if x >= threshold else -1)\n",
    "        # Add weighted prediction to final prediction\n",
    "        final_predictions = [fp + alpha * pred for fp, pred in zip(final_predictions, predictions)]\n",
    "    # The final prediction is based on the sign of the aggregated score\n",
    "    return [1 if fp > 0 else 0 for fp in final_predictions]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa65abbe-1150-45e9-8d24-d5213db4ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 done\n",
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n",
      "Iteration 4 done\n",
      "Iteration 5 done\n",
      "Iteration 6 done\n",
      "Iteration 7 done\n",
      "Iteration 8 done\n",
      "Iteration 9 done\n",
      "Test Accuracy for this dataset: 11.11%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Function to test the implementation with a given dataset\n",
    "def test_adaboost(data, target):\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Prepare training DataFrame\n",
    "    train_df = pd.DataFrame(X_train, columns=[f'feature_{i+1}' for i in range(X_train.shape[1])])\n",
    "    train_df['target'] = y_train\n",
    "    train_df['weights'] = 1 / len(train_df)\n",
    "    \n",
    "    # AdaBoost Training Process\n",
    "    individual_learners = []\n",
    "    for i in range(10):\n",
    "        max_feature, max_gain, max_threshold = get_best_stump(train_df)\n",
    "        total_error, train_df = calculate_error(train_df, max_feature, max_threshold)\n",
    "        amount_of_say = calculate_amount_of_say(total_error)\n",
    "        train_df = reweight(amount_of_say, train_df)\n",
    "        individual_learners.append((max_feature, max_threshold, amount_of_say))\n",
    "        print(f'Iteration {i} done')\n",
    "    \n",
    "    # Prepare test DataFrame\n",
    "    test_df = pd.DataFrame(X_test, columns=[f'feature_{i+1}' for i in range(X_test.shape[1])])\n",
    "    test_df['target'] = y_test\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    test_df['predictions'] = predict(test_df, individual_learners)\n",
    "    \n",
    "    # Measure accuracy on the test set\n",
    "    accuracy = (test_df['predictions'] == test_df['target']).mean()\n",
    "    print(f\"Test Accuracy for this dataset: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test with synthetic classification dataset\n",
    "X, y = make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
    "test_adaboost(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b4c05-885c-4369-bb77-fea3438ba83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test with Iris dataset (consider only binary classification)\n",
    "iris = load_iris()\n",
    "iris_data = iris.data[iris.target != 2]\n",
    "iris_target = iris.target[iris.target != 2]\n",
    "test_adaboost(iris_data, iris_target)\n",
    "\n",
    "# Test with Wine dataset (use only binary classification for simplicity)\n",
    "wine = load_wine()\n",
    "wine_data = wine.data[wine.target != 2]\n",
    "wine_target = wine.target[wine.target != 2]\n",
    "test_adaboost(wine_data, wine_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d2812-c659-43bb-ace9-09c7caf724bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
