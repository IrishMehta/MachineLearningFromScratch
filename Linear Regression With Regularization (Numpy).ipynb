{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7108867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Multi-linear regression\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self,lr,iterations,alpha=0,lamda=0,shuffle=False):\n",
    "        self.learning_rate=lr\n",
    "        self.alpha=alpha\n",
    "        self.lamda=lamda\n",
    "\n",
    "        self.iter=iterations\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Linear Regression Model initialized with learning Rate of {self.learning_rate} over {self.iter} iterations and L1, L2 regularization of {self.alpha}, {self.lamda} respectively'\n",
    "    \n",
    "    def random_init(self):\n",
    "        self.coeff=np.ones(shape=(len(self.X[0])+1,1))\n",
    "        \n",
    "    def loss_calculation(self):\n",
    "        self.y_pred=[]\n",
    "        \n",
    "        #modifying X to now have one extra column of bias term\n",
    "        dot_temp=np.hstack([self.X, np.ones(shape=(len(self.X), 1))])\n",
    "        self.y_pred=dot_temp.dot(self.coeff)\n",
    "        \n",
    "        error=np.mean((self.y-self.y_pred)**2)\n",
    "        \n",
    "        #L1 Regularization\n",
    "        l1_reg=self.alpha*np.sum(np.abs(self.coeff[:-1]))\n",
    "        \n",
    "        #L2 Regularization\n",
    "        l2_reg=self.lamda*np.sum(self.coeff[:-1]**2)\n",
    "        \n",
    "        #Calculating loss value\n",
    "        self.loss= error+l1_reg+l2_reg\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        residual_error = (self.y - self.y_pred)\n",
    "\n",
    "        l1_reg_derivative= self.alpha*np.sign(self.coeff[:-1])\n",
    "        l2_reg_derivative= 2*self.lamda*self.coeff[:-1]\n",
    "        \n",
    "        # Dot product required because we are multiplying two vectors; Flatten required to match dimensions\n",
    "        self.dl_by_da = -2 * (self.X.T.dot(residual_error)).flatten()  / len(self.X) + l1_reg_derivative.flatten() + l2_reg_derivative.flatten()\n",
    "        self.coeff[:-1]=self.coeff[:-1] - self.learning_rate * self.dl_by_da[:, np.newaxis]\n",
    "        \n",
    "        self.dl_by_db=-2*np.mean(residual_error)\n",
    "        self.coeff[-1]= self.coeff[-1]-self.learning_rate*self.dl_by_db\n",
    "\n",
    "    def display_eq(self):\n",
    "        temp_str='y='\n",
    "        for index, coeff in enumerate(self.coeff[:-1]):\n",
    "            temp_str += f'{coeff[0]}*X{index} + '\n",
    "        temp_str += str(self.coeff[-1][0])\n",
    "        print(temp_str)\n",
    "            \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X=X\n",
    "        self.y=y.reshape(-1,1)\n",
    "\n",
    "        self.random_init()\n",
    "        self.y_pred=[]\n",
    "\n",
    "        for i in range(self.iter):\n",
    "            self.loss_calculation()\n",
    "            self.gradient_descent()\n",
    "            print(f\"Epoch {i+1}/{self.iter} - Loss: {self.loss}\")\n",
    "        self.loss_calculation()\n",
    "        \n",
    "        self.display_eq()\n",
    "        \n",
    "    def predict(self, x_input):\n",
    "        \n",
    "        modified_x_input=np.hstack([x_input, np.ones(shape=(len(x_input), 1))])\n",
    "        test_predictions=modified_x_input.dot(self.coeff)\n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7185ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Loss: 5.901090014585407\n",
      "Epoch 2/1000 - Loss: 5.887730403301684\n",
      "Epoch 3/1000 - Loss: 5.874457454423693\n",
      "Epoch 4/1000 - Loss: 5.861269214369093\n",
      "Epoch 5/1000 - Loss: 5.848163791924205\n",
      "Epoch 6/1000 - Loss: 5.83513935618338\n",
      "Epoch 7/1000 - Loss: 5.822194134556653\n",
      "Epoch 8/1000 - Loss: 5.809326410843376\n",
      "Epoch 9/1000 - Loss: 5.796534523369701\n",
      "Epoch 10/1000 - Loss: 5.7838168631877345\n",
      "Epoch 11/1000 - Loss: 5.771171872334381\n",
      "Epoch 12/1000 - Loss: 5.758598042147845\n",
      "Epoch 13/1000 - Loss: 5.746093911639921\n",
      "Epoch 14/1000 - Loss: 5.7336580659221905\n",
      "Epoch 15/1000 - Loss: 5.721289134684376\n",
      "Epoch 16/1000 - Loss: 5.708985790723075\n",
      "Epoch 17/1000 - Loss: 5.69674674851926\n",
      "Epoch 18/1000 - Loss: 5.684570762862883\n",
      "Epoch 19/1000 - Loss: 5.672456627523054\n",
      "Epoch 20/1000 - Loss: 5.660403173962255\n",
      "Epoch 21/1000 - Loss: 5.648409270093166\n",
      "Epoch 22/1000 - Loss: 5.636473819076654\n",
      "Epoch 23/1000 - Loss: 5.624595758159591\n",
      "Epoch 24/1000 - Loss: 5.61277405755116\n",
      "Epoch 25/1000 - Loss: 5.601007719336389\n",
      "Epoch 26/1000 - Loss: 5.589295776425666\n",
      "Epoch 27/1000 - Loss: 5.577637291539048\n",
      "Epoch 28/1000 - Loss: 5.566031356224235\n",
      "Epoch 29/1000 - Loss: 5.554477089907027\n",
      "Epoch 30/1000 - Loss: 5.542973638973279\n",
      "Epoch 31/1000 - Loss: 5.531520175881227\n",
      "Epoch 32/1000 - Loss: 5.520115898303241\n",
      "Epoch 33/1000 - Loss: 5.508760028295976\n",
      "Epoch 34/1000 - Loss: 5.497451811498035\n",
      "Epoch 35/1000 - Loss: 5.486190516354183\n",
      "Epoch 36/1000 - Loss: 5.474975433365269\n",
      "Epoch 37/1000 - Loss: 5.463805874362974\n",
      "Epoch 38/1000 - Loss: 5.4526811718085995\n",
      "Epoch 39/1000 - Loss: 5.44160067811505\n",
      "Epoch 40/1000 - Loss: 5.430563764991292\n",
      "Epoch 41/1000 - Loss: 5.419569822808511\n",
      "Epoch 42/1000 - Loss: 5.4086182599872545\n",
      "Epoch 43/1000 - Loss: 5.397708502404884\n",
      "Epoch 44/1000 - Loss: 5.386839992822623\n",
      "Epoch 45/1000 - Loss: 5.376012190331612\n",
      "Epoch 46/1000 - Loss: 5.365224569817264\n",
      "Epoch 47/1000 - Loss: 5.354476621441397\n",
      "Epoch 48/1000 - Loss: 5.3437678501414805\n",
      "Epoch 49/1000 - Loss: 5.333097775146478\n",
      "Epoch 50/1000 - Loss: 5.322465929508716\n",
      "Epoch 51/1000 - Loss: 5.311871859651251\n",
      "Epoch 52/1000 - Loss: 5.30131512493022\n",
      "Epoch 53/1000 - Loss: 5.290795297211682\n",
      "Epoch 54/1000 - Loss: 5.280311960462472\n",
      "Epoch 55/1000 - Loss: 5.269864710354583\n",
      "Epoch 56/1000 - Loss: 5.259453153882666\n",
      "Epoch 57/1000 - Loss: 5.249076908994173\n",
      "Epoch 58/1000 - Loss: 5.238735604231749\n",
      "Epoch 59/1000 - Loss: 5.228428878387473\n",
      "Epoch 60/1000 - Loss: 5.218156380168511\n",
      "Epoch 61/1000 - Loss: 5.207917767873871\n",
      "Epoch 62/1000 - Loss: 5.197712709081831\n",
      "Epoch 63/1000 - Loss: 5.187540880347721\n",
      "Epoch 64/1000 - Loss: 5.177401966911702\n",
      "Epoch 65/1000 - Loss: 5.167295662416226\n",
      "Epoch 66/1000 - Loss: 5.157221668632825\n",
      "Epoch 67/1000 - Loss: 5.147179695197958\n",
      "Epoch 68/1000 - Loss: 5.13716945935759\n",
      "Epoch 69/1000 - Loss: 5.1271906857202225\n",
      "Epoch 70/1000 - Loss: 5.117243106018094\n",
      "Epoch 71/1000 - Loss: 5.107326458876287\n",
      "Epoch 72/1000 - Loss: 5.097440489589454\n",
      "Epoch 73/1000 - Loss: 5.087584949905958\n",
      "Epoch 74/1000 - Loss: 5.077759597819124\n",
      "Epoch 75/1000 - Loss: 5.067964197365418\n",
      "Epoch 76/1000 - Loss: 5.058198518429276\n",
      "Epoch 77/1000 - Loss: 5.04846233655442\n",
      "Epoch 78/1000 - Loss: 5.0387554327613735\n",
      "Epoch 79/1000 - Loss: 5.029077593371053\n",
      "Epoch 80/1000 - Loss: 5.019428609834155\n",
      "Epoch 81/1000 - Loss: 5.0098082785662115\n",
      "Epoch 82/1000 - Loss: 5.000216400788077\n",
      "Epoch 83/1000 - Loss: 4.990652782371709\n",
      "Epoch 84/1000 - Loss: 4.981117233691023\n",
      "Epoch 85/1000 - Loss: 4.971609569477703\n",
      "Epoch 86/1000 - Loss: 4.962129608681749\n",
      "Epoch 87/1000 - Loss: 4.952677174336658\n",
      "Epoch 88/1000 - Loss: 4.943252093429048\n",
      "Epoch 89/1000 - Loss: 4.933854196772581\n",
      "Epoch 90/1000 - Loss: 4.92448331888607\n",
      "Epoch 91/1000 - Loss: 4.915139297875605\n",
      "Epoch 92/1000 - Loss: 4.905821975320561\n",
      "Epoch 93/1000 - Loss: 4.896531196163386\n",
      "Epoch 94/1000 - Loss: 4.887266808603028\n",
      "Epoch 95/1000 - Loss: 4.878028663991862\n",
      "Epoch 96/1000 - Loss: 4.868816616736046\n",
      "Epoch 97/1000 - Loss: 4.859630524199153\n",
      "Epoch 98/1000 - Loss: 4.850470246608978\n",
      "Epoch 99/1000 - Loss: 4.8413356469674405\n",
      "Epoch 100/1000 - Loss: 4.832226590963434\n",
      "Epoch 101/1000 - Loss: 4.8231429468885745\n",
      "Epoch 102/1000 - Loss: 4.814084585555712\n",
      "Epoch 103/1000 - Loss: 4.805051380220147\n",
      "Epoch 104/1000 - Loss: 4.796043206503423\n",
      "Epoch 105/1000 - Loss: 4.787059942319656\n",
      "Epoch 106/1000 - Loss: 4.778101467804272\n",
      "Epoch 107/1000 - Loss: 4.76916766524511\n",
      "Epoch 108/1000 - Loss: 4.760258419015784\n",
      "Epoch 109/1000 - Loss: 4.751373615511254\n",
      "Epoch 110/1000 - Loss: 4.7425131430855\n",
      "Epoch 111/1000 - Loss: 4.733676891991272\n",
      "Epoch 112/1000 - Loss: 4.724864754321816\n",
      "Epoch 113/1000 - Loss: 4.716076623954504\n",
      "Epoch 114/1000 - Loss: 4.707312396496347\n",
      "Epoch 115/1000 - Loss: 4.698571969231279\n",
      "Epoch 116/1000 - Loss: 4.689855241069186\n",
      "Epoch 117/1000 - Loss: 4.681162112496618\n",
      "Epoch 118/1000 - Loss: 4.672492485529107\n",
      "Epoch 119/1000 - Loss: 4.663846263665063\n",
      "Epoch 120/1000 - Loss: 4.655223351841189\n",
      "Epoch 121/1000 - Loss: 4.646623656389356\n",
      "Epoch 122/1000 - Loss: 4.6380470849948905\n",
      "Epoch 123/1000 - Loss: 4.629493546656249\n",
      "Epoch 124/1000 - Loss: 4.620962951646\n",
      "Epoch 125/1000 - Loss: 4.612455211473095\n",
      "Epoch 126/1000 - Loss: 4.603970238846377\n",
      "Epoch 127/1000 - Loss: 4.595507947639293\n",
      "Epoch 128/1000 - Loss: 4.58706825285575\n",
      "Epoch 129/1000 - Loss: 4.578651070597106\n",
      "Epoch 130/1000 - Loss: 4.5702563180302285\n",
      "Epoch 131/1000 - Loss: 4.561883913356624\n",
      "Epoch 132/1000 - Loss: 4.553533775782542\n",
      "Epoch 133/1000 - Loss: 4.545205825490098\n",
      "Epoch 134/1000 - Loss: 4.536899983609313\n",
      "Epoch 135/1000 - Loss: 4.528616172191084\n",
      "Epoch 136/1000 - Loss: 4.520354314181029\n",
      "Epoch 137/1000 - Loss: 4.512114333394201\n",
      "Epoch 138/1000 - Loss: 4.503896154490607\n",
      "Epoch 139/1000 - Loss: 4.495699702951555\n",
      "Epoch 140/1000 - Loss: 4.487524905056744\n",
      "Epoch 141/1000 - Loss: 4.479371687862125\n",
      "Epoch 142/1000 - Loss: 4.471239979178479\n",
      "Epoch 143/1000 - Loss: 4.4631297075506815\n",
      "Epoch 144/1000 - Loss: 4.455040802237659\n",
      "Epoch 145/1000 - Loss: 4.446973193192992\n",
      "Epoch 146/1000 - Loss: 4.438926811046147\n",
      "Epoch 147/1000 - Loss: 4.430901587084325\n",
      "Epoch 148/1000 - Loss: 4.422897453234895\n",
      "Epoch 149/1000 - Loss: 4.414914342048405\n",
      "Epoch 150/1000 - Loss: 4.406952186682142\n",
      "Epoch 151/1000 - Loss: 4.399010920884218\n",
      "Epoch 152/1000 - Loss: 4.391090478978192\n",
      "Epoch 153/1000 - Loss: 4.383190795848171\n",
      "Epoch 154/1000 - Loss: 4.375311806924403\n",
      "Epoch 155/1000 - Loss: 4.367453448169336\n",
      "Epoch 156/1000 - Loss: 4.359615656064128\n",
      "Epoch 157/1000 - Loss: 4.3517983675955945\n",
      "Epoch 158/1000 - Loss: 4.344001520243574\n",
      "Epoch 159/1000 - Loss: 4.336225051968706\n",
      "Epoch 160/1000 - Loss: 4.328468901200598\n",
      "Epoch 161/1000 - Loss: 4.3207330068263845\n",
      "Epoch 162/1000 - Loss: 4.313017308179639\n",
      "Epoch 163/1000 - Loss: 4.305321745029656\n",
      "Epoch 164/1000 - Loss: 4.297646257571075\n",
      "Epoch 165/1000 - Loss: 4.28999078641383\n",
      "Epoch 166/1000 - Loss: 4.282355272573434\n",
      "Epoch 167/1000 - Loss: 4.274739657461562\n",
      "Epoch 168/1000 - Loss: 4.267143882876949\n",
      "Epoch 169/1000 - Loss: 4.2595678909965695\n",
      "Epoch 170/1000 - Loss: 4.252011624367096\n",
      "Epoch 171/1000 - Loss: 4.244475025896651\n",
      "Epoch 172/1000 - Loss: 4.236958038846789\n",
      "Epoch 173/1000 - Loss: 4.229460606824767\n",
      "Epoch 174/1000 - Loss: 4.221982673776036\n",
      "Epoch 175/1000 - Loss: 4.21452418397699\n",
      "Epoch 176/1000 - Loss: 4.207085082027929\n",
      "Epoch 177/1000 - Loss: 4.199665312846252\n",
      "Epoch 178/1000 - Loss: 4.192264821659875\n",
      "Epoch 179/1000 - Loss: 4.184883554000837\n",
      "Epoch 180/1000 - Loss: 4.177521455699123\n",
      "Epoch 181/1000 - Loss: 4.170178472876677\n",
      "Epoch 182/1000 - Loss: 4.1628545519416065\n",
      "Epoch 183/1000 - Loss: 4.155549639582557\n",
      "Epoch 184/1000 - Loss: 4.148263682763284\n",
      "Epoch 185/1000 - Loss: 4.140996628717377\n",
      "Epoch 186/1000 - Loss: 4.1337484249431515\n",
      "Epoch 187/1000 - Loss: 4.126519019198706\n",
      "Epoch 188/1000 - Loss: 4.119308359497132\n",
      "Epoch 189/1000 - Loss: 4.112116394101871\n",
      "Epoch 190/1000 - Loss: 4.10494307152221\n",
      "Epoch 191/1000 - Loss: 4.0977883405089335\n",
      "Epoch 192/1000 - Loss: 4.09065215005008\n",
      "Epoch 193/1000 - Loss: 4.083534449366871\n",
      "Epoch 194/1000 - Loss: 4.07643518790972\n",
      "Epoch 195/1000 - Loss: 4.069354315354402\n",
      "Epoch 196/1000 - Loss: 4.062291781598316\n",
      "Epoch 197/1000 - Loss: 4.0552475367568706\n",
      "Epoch 198/1000 - Loss: 4.0482215311599825\n",
      "Epoch 199/1000 - Loss: 4.041213715348675\n",
      "Epoch 200/1000 - Loss: 4.034224040071789\n",
      "Epoch 201/1000 - Loss: 4.027252456282772\n",
      "Epoch 202/1000 - Loss: 4.020298915136601\n",
      "Epoch 203/1000 - Loss: 4.013363367986755\n",
      "Epoch 204/1000 - Loss: 4.006445766382312\n",
      "Epoch 205/1000 - Loss: 3.9995460620651193\n",
      "Epoch 206/1000 - Loss: 3.9926642069670444\n",
      "Epoch 207/1000 - Loss: 3.985800153207314\n",
      "Epoch 208/1000 - Loss: 3.978953853089931\n",
      "Epoch 209/1000 - Loss: 3.9721252591011664\n",
      "Epoch 210/1000 - Loss: 3.9653143239071253\n",
      "Epoch 211/1000 - Loss: 3.9585210003513827\n",
      "Epoch 212/1000 - Loss: 3.9517452414526937\n",
      "Epoch 213/1000 - Loss: 3.9449870004027603\n",
      "Epoch 214/1000 - Loss: 3.9382462305640784\n",
      "Epoch 215/1000 - Loss: 3.9315228854678277\n",
      "Epoch 216/1000 - Loss: 3.9248169188118416\n",
      "Epoch 217/1000 - Loss: 3.9181282844586187\n",
      "Epoch 218/1000 - Loss: 3.911456936433403\n",
      "Epoch 219/1000 - Loss: 3.904802828922313\n",
      "Epoch 220/1000 - Loss: 3.8981659162705222\n",
      "Epoch 221/1000 - Loss: 3.891546152980498\n",
      "Epoch 222/1000 - Loss: 3.8849434937102814\n",
      "Epoch 223/1000 - Loss: 3.878357893271821\n",
      "Epoch 224/1000 - Loss: 3.8717893066293483\n",
      "Epoch 225/1000 - Loss: 3.8652376888978037\n",
      "Epoch 226/1000 - Loss: 3.858702995341299\n",
      "Epoch 227/1000 - Loss: 3.852185181371628\n",
      "Epoch 228/1000 - Loss: 3.845684202546813\n",
      "Epoch 229/1000 - Loss: 3.8392000145696943\n",
      "Epoch 230/1000 - Loss: 3.832732573286553\n",
      "Epoch 231/1000 - Loss: 3.8262818346857745\n",
      "Epoch 232/1000 - Loss: 3.8198477548965513\n",
      "Epoch 233/1000 - Loss: 3.813430290187605\n",
      "Epoch 234/1000 - Loss: 3.8070293969659605\n",
      "Epoch 235/1000 - Loss: 3.800645031775739\n",
      "Epoch 236/1000 - Loss: 3.79427715129699\n",
      "Epoch 237/1000 - Loss: 3.7879257123445464\n",
      "Epoch 238/1000 - Loss: 3.7815906718669154\n",
      "Epoch 239/1000 - Loss: 3.7752719869451927\n",
      "Epoch 240/1000 - Loss: 3.768969614792008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000 - Loss: 3.762683512750495\n",
      "Epoch 242/1000 - Loss: 3.7564136382932825\n",
      "Epoch 243/1000 - Loss: 3.75015994902152\n",
      "Epoch 244/1000 - Loss: 3.7439224026639173\n",
      "Epoch 245/1000 - Loss: 3.737700957075816\n",
      "Epoch 246/1000 - Loss: 3.7314955702382786\n",
      "Epoch 247/1000 - Loss: 3.7253062002571937\n",
      "Epoch 248/1000 - Loss: 3.7191328053624164\n",
      "Epoch 249/1000 - Loss: 3.7129753439069146\n",
      "Epoch 250/1000 - Loss: 3.7068337743659487\n",
      "Epoch 251/1000 - Loss: 3.700708055336255\n",
      "Epoch 252/1000 - Loss: 3.694598145535258\n",
      "Epoch 253/1000 - Loss: 3.688504003800303\n",
      "Epoch 254/1000 - Loss: 3.682425589087895\n",
      "Epoch 255/1000 - Loss: 3.676362860472961\n",
      "Epoch 256/1000 - Loss: 3.6703157771481325\n",
      "Epoch 257/1000 - Loss: 3.6642842984230324\n",
      "Epoch 258/1000 - Loss: 3.6582683837235894\n",
      "Epoch 259/1000 - Loss: 3.652267992591357\n",
      "Epoch 260/1000 - Loss: 3.6462830846828553\n",
      "Epoch 261/1000 - Loss: 3.6403136197689188\n",
      "Epoch 262/1000 - Loss: 3.6343595577340624\n",
      "Epoch 263/1000 - Loss: 3.6284208585758595\n",
      "Epoch 264/1000 - Loss: 3.6224974824043312\n",
      "Epoch 265/1000 - Loss: 3.6165893894413506\n",
      "Epoch 266/1000 - Loss: 3.6106965400200526\n",
      "Epoch 267/1000 - Loss: 3.6048188945842656\n",
      "Epoch 268/1000 - Loss: 3.598956413687941\n",
      "Epoch 269/1000 - Loss: 3.5931090579946057\n",
      "Epoch 270/1000 - Loss: 3.5872767882768155\n",
      "Epoch 271/1000 - Loss: 3.5814595654156265\n",
      "Epoch 272/1000 - Loss: 3.5756573504000695\n",
      "Epoch 273/1000 - Loss: 3.569870104326636\n",
      "Epoch 274/1000 - Loss: 3.5640977883987754\n",
      "Epoch 275/1000 - Loss: 3.558340363926401\n",
      "Epoch 276/1000 - Loss: 3.5525977923253977\n",
      "Epoch 277/1000 - Loss: 3.546870035117149\n",
      "Epoch 278/1000 - Loss: 3.541157053928064\n",
      "Epoch 279/1000 - Loss: 3.535458810489114\n",
      "Epoch 280/1000 - Loss: 3.5297752666353768\n",
      "Epoch 281/1000 - Loss: 3.5241063843055924\n",
      "Epoch 282/1000 - Loss: 3.5184521255417174\n",
      "Epoch 283/1000 - Loss: 3.5128124524884936\n",
      "Epoch 284/1000 - Loss: 3.507187327393021\n",
      "Epoch 285/1000 - Loss: 3.501576712604335\n",
      "Epoch 286/1000 - Loss: 3.495980570572993\n",
      "Epoch 287/1000 - Loss: 3.490398863850669\n",
      "Epoch 288/1000 - Loss: 3.484831555089743\n",
      "Epoch 289/1000 - Loss: 3.4792786070429145\n",
      "Epoch 290/1000 - Loss: 3.473739982562803\n",
      "Epoch 291/1000 - Loss: 3.4682156446015653\n",
      "Epoch 292/1000 - Loss: 3.4627055562105173\n",
      "Epoch 293/1000 - Loss: 3.4572096805397536\n",
      "Epoch 294/1000 - Loss: 3.4517279808377794\n",
      "Epoch 295/1000 - Loss: 3.4462604204511478\n",
      "Epoch 296/1000 - Loss: 3.4408069628240923\n",
      "Epoch 297/1000 - Loss: 3.4353675714981744\n",
      "Epoch 298/1000 - Loss: 3.429942210111932\n",
      "Epoch 299/1000 - Loss: 3.4245308424005283\n",
      "Epoch 300/1000 - Loss: 3.419133432195411\n",
      "Epoch 301/1000 - Loss: 3.4137499434239698\n",
      "Epoch 302/1000 - Loss: 3.4083803401092045\n",
      "Epoch 303/1000 - Loss: 3.403024586369388\n",
      "Epoch 304/1000 - Loss: 3.39768264641774\n",
      "Epoch 305/1000 - Loss: 3.392354484562105\n",
      "Epoch 306/1000 - Loss: 3.3870400652046273\n",
      "Epoch 307/1000 - Loss: 3.381739352841432\n",
      "Epoch 308/1000 - Loss: 3.3764523120623164\n",
      "Epoch 309/1000 - Loss: 3.371178907550434\n",
      "Epoch 310/1000 - Loss: 3.3659191040819834\n",
      "Epoch 311/1000 - Loss: 3.360672866525911\n",
      "Epoch 312/1000 - Loss: 3.3554401598436034\n",
      "Epoch 313/1000 - Loss: 3.3502209490885875\n",
      "Epoch 314/1000 - Loss: 3.345015199406235\n",
      "Epoch 315/1000 - Loss: 3.3398228760334714\n",
      "Epoch 316/1000 - Loss: 3.3346439442984797\n",
      "Epoch 317/1000 - Loss: 3.3294783696204138\n",
      "Epoch 318/1000 - Loss: 3.324326117509114\n",
      "Epoch 319/1000 - Loss: 3.3191871535648216\n",
      "Epoch 320/1000 - Loss: 3.314061443477897\n",
      "Epoch 321/1000 - Loss: 3.3089489530285414\n",
      "Epoch 322/1000 - Loss: 3.3038496480865214\n",
      "Epoch 323/1000 - Loss: 3.2987634946108906\n",
      "Epoch 324/1000 - Loss: 3.293690458649722\n",
      "Epoch 325/1000 - Loss: 3.288630506339835\n",
      "Epoch 326/1000 - Loss: 3.283583603906526\n",
      "Epoch 327/1000 - Loss: 3.278549717663305\n",
      "Epoch 328/1000 - Loss: 3.27352881401163\n",
      "Epoch 329/1000 - Loss: 3.268520859440644\n",
      "Epoch 330/1000 - Loss: 3.263525820526915\n",
      "Epoch 331/1000 - Loss: 3.258543663934177\n",
      "Epoch 332/1000 - Loss: 3.253574356413073\n",
      "Epoch 333/1000 - Loss: 3.2486178648009028\n",
      "Epoch 334/1000 - Loss: 3.2436741560213616\n",
      "Epoch 335/1000 - Loss: 3.2387431970842973\n",
      "Epoch 336/1000 - Loss: 3.233824955085454\n",
      "Epoch 337/1000 - Loss: 3.228919397206227\n",
      "Epoch 338/1000 - Loss: 3.224026490713412\n",
      "Epoch 339/1000 - Loss: 3.219146202958963\n",
      "Epoch 340/1000 - Loss: 3.2142785013797446\n",
      "Epoch 341/1000 - Loss: 3.209423353497293\n",
      "Epoch 342/1000 - Loss: 3.2045807269175732\n",
      "Epoch 343/1000 - Loss: 3.199750589330736\n",
      "Epoch 344/1000 - Loss: 3.1949329085108853\n",
      "Epoch 345/1000 - Loss: 3.190127652315835\n",
      "Epoch 346/1000 - Loss: 3.1853347886868772\n",
      "Epoch 347/1000 - Loss: 3.180554285648546\n",
      "Epoch 348/1000 - Loss: 3.1757861113083834\n",
      "Epoch 349/1000 - Loss: 3.171030233856707\n",
      "Epoch 350/1000 - Loss: 3.1662866215663827\n",
      "Epoch 351/1000 - Loss: 3.1615552427925886\n",
      "Epoch 352/1000 - Loss: 3.1568360659725907\n",
      "Epoch 353/1000 - Loss: 3.1521290596255143\n",
      "Epoch 354/1000 - Loss: 3.1474341923521174\n",
      "Epoch 355/1000 - Loss: 3.1427514328345665\n",
      "Epoch 356/1000 - Loss: 3.1380807498362104\n",
      "Epoch 357/1000 - Loss: 3.133422112201356\n",
      "Epoch 358/1000 - Loss: 3.128775488855052\n",
      "Epoch 359/1000 - Loss: 3.124140848802861\n",
      "Epoch 360/1000 - Loss: 3.119518161130642\n",
      "Epoch 361/1000 - Loss: 3.1149073950043316\n",
      "Epoch 362/1000 - Loss: 3.1103085196697258\n",
      "Epoch 363/1000 - Loss: 3.105721504452262\n",
      "Epoch 364/1000 - Loss: 3.101146318756804\n",
      "Epoch 365/1000 - Loss: 3.0965829320674247\n",
      "Epoch 366/1000 - Loss: 3.0920313139471918\n",
      "Epoch 367/1000 - Loss: 3.087491434037956\n",
      "Epoch 368/1000 - Loss: 3.0829632620601357\n",
      "Epoch 369/1000 - Loss: 3.0784467678125096\n",
      "Epoch 370/1000 - Loss: 3.073941921172\n",
      "Epoch 371/1000 - Loss: 3.069448692093465\n",
      "Epoch 372/1000 - Loss: 3.064967050609492\n",
      "Epoch 373/1000 - Loss: 3.0604969668301845\n",
      "Epoch 374/1000 - Loss: 3.056038410942956\n",
      "Epoch 375/1000 - Loss: 3.051591353212327\n",
      "Epoch 376/1000 - Loss: 3.047155763979711\n",
      "Epoch 377/1000 - Loss: 3.042731613663216\n",
      "Epoch 378/1000 - Loss: 3.0383188727574373\n",
      "Epoch 379/1000 - Loss: 3.0339175118332533\n",
      "Epoch 380/1000 - Loss: 3.0295275015376224\n",
      "Epoch 381/1000 - Loss: 3.025148812593383\n",
      "Epoch 382/1000 - Loss: 3.020781415799048\n",
      "Epoch 383/1000 - Loss: 3.0164252820286057\n",
      "Epoch 384/1000 - Loss: 3.0120803822313205\n",
      "Epoch 385/1000 - Loss: 3.0077466874315335\n",
      "Epoch 386/1000 - Loss: 3.003424168728461\n",
      "Epoch 387/1000 - Loss: 2.999112797295998\n",
      "Epoch 388/1000 - Loss: 2.9948125443825235\n",
      "Epoch 389/1000 - Loss: 2.9905233813106973\n",
      "Epoch 390/1000 - Loss: 2.9862452794772705\n",
      "Epoch 391/1000 - Loss: 2.9819782103528856\n",
      "Epoch 392/1000 - Loss: 2.9777221454818834\n",
      "Epoch 393/1000 - Loss: 2.9734770564821087\n",
      "Epoch 394/1000 - Loss: 2.9692429150447186\n",
      "Epoch 395/1000 - Loss: 2.965019692933984\n",
      "Epoch 396/1000 - Loss: 2.960807361987104\n",
      "Epoch 397/1000 - Loss: 2.9566058941140096\n",
      "Epoch 398/1000 - Loss: 2.9524152612971757\n",
      "Epoch 399/1000 - Loss: 2.9482354355914278\n",
      "Epoch 400/1000 - Loss: 2.944066389123754\n",
      "Epoch 401/1000 - Loss: 2.9399080940931155\n",
      "Epoch 402/1000 - Loss: 2.9357605227702592\n",
      "Epoch 403/1000 - Loss: 2.931623647497526\n",
      "Epoch 404/1000 - Loss: 2.927497440688666\n",
      "Epoch 405/1000 - Loss: 2.923381874828652\n",
      "Epoch 406/1000 - Loss: 2.9192769224734914\n",
      "Epoch 407/1000 - Loss: 2.915182556250042\n",
      "Epoch 408/1000 - Loss: 2.9110987488558266\n",
      "Epoch 409/1000 - Loss: 2.9070254730588476\n",
      "Epoch 410/1000 - Loss: 2.902962701697402\n",
      "Epoch 411/1000 - Loss: 2.8989104076799004\n",
      "Epoch 412/1000 - Loss: 2.8948685639846823\n",
      "Epoch 413/1000 - Loss: 2.8908371436598346\n",
      "Epoch 414/1000 - Loss: 2.886816119823008\n",
      "Epoch 415/1000 - Loss: 2.8828054656612387\n",
      "Epoch 416/1000 - Loss: 2.878805154430763\n",
      "Epoch 417/1000 - Loss: 2.874815159456843\n",
      "Epoch 418/1000 - Loss: 2.870835454133581\n",
      "Epoch 419/1000 - Loss: 2.866866011923743\n",
      "Epoch 420/1000 - Loss: 2.8629068063585796\n",
      "Epoch 421/1000 - Loss: 2.8589578110376506\n",
      "Epoch 422/1000 - Loss: 2.8550189996286415\n",
      "Epoch 423/1000 - Loss: 2.8510903458671915\n",
      "Epoch 424/1000 - Loss: 2.847171823556712\n",
      "Epoch 425/1000 - Loss: 2.8432634065682163\n",
      "Epoch 426/1000 - Loss: 2.8393650688401375\n",
      "Epoch 427/1000 - Loss: 2.8354767843781605\n",
      "Epoch 428/1000 - Loss: 2.831598527255041\n",
      "Epoch 429/1000 - Loss: 2.827730271610432\n",
      "Epoch 430/1000 - Loss: 2.8238719916507162\n",
      "Epoch 431/1000 - Loss: 2.820023661648824\n",
      "Epoch 432/1000 - Loss: 2.816185255944068\n",
      "Epoch 433/1000 - Loss: 2.8123567489419665\n",
      "Epoch 434/1000 - Loss: 2.808538115114074\n",
      "Epoch 435/1000 - Loss: 2.8047293289978077\n",
      "Epoch 436/1000 - Loss: 2.8009303651962774\n",
      "Epoch 437/1000 - Loss: 2.7971411983781196\n",
      "Epoch 438/1000 - Loss: 2.79336180327732\n",
      "Epoch 439/1000 - Loss: 2.7895921546930484\n",
      "Epoch 440/1000 - Loss: 2.78583222748949\n",
      "Epoch 441/1000 - Loss: 2.7820819965956765\n",
      "Epoch 442/1000 - Loss: 2.7783414370053157\n",
      "Epoch 443/1000 - Loss: 2.7746105237766288\n",
      "Epoch 444/1000 - Loss: 2.770889232032178\n",
      "Epoch 445/1000 - Loss: 2.7671775369587035\n",
      "Epoch 446/1000 - Loss: 2.7634754138069546\n",
      "Epoch 447/1000 - Loss: 2.759782837891527\n",
      "Epoch 448/1000 - Loss: 2.7560997845906954\n",
      "Epoch 449/1000 - Loss: 2.7524262293462494\n",
      "Epoch 450/1000 - Loss: 2.748762147663329\n",
      "Epoch 451/1000 - Loss: 2.7451075151102593\n",
      "Epoch 452/1000 - Loss: 2.74146230731839\n",
      "Epoch 453/1000 - Loss: 2.737826499981929\n",
      "Epoch 454/1000 - Loss: 2.734200068857782\n",
      "Epoch 455/1000 - Loss: 2.7305829897653897\n",
      "Epoch 456/1000 - Loss: 2.7269752385865655\n",
      "Epoch 457/1000 - Loss: 2.7233767912653355\n",
      "Epoch 458/1000 - Loss: 2.7197876238077763\n",
      "Epoch 459/1000 - Loss: 2.7162077122818538\n",
      "Epoch 460/1000 - Loss: 2.712637032817266\n",
      "Epoch 461/1000 - Loss: 2.709075561605281\n",
      "Epoch 462/1000 - Loss: 2.705523274898578\n",
      "Epoch 463/1000 - Loss: 2.7019801490110895\n",
      "Epoch 464/1000 - Loss: 2.698446160317843\n",
      "Epoch 465/1000 - Loss: 2.6949212852548032\n",
      "Epoch 466/1000 - Loss: 2.6914055003187105\n",
      "Epoch 467/1000 - Loss: 2.6878987820669313\n",
      "Epoch 468/1000 - Loss: 2.6844011071172953\n",
      "Epoch 469/1000 - Loss: 2.6809124521479415\n",
      "Epoch 470/1000 - Loss: 2.6774327938971627\n",
      "Epoch 471/1000 - Loss: 2.673962109163248\n",
      "Epoch 472/1000 - Loss: 2.6705003748043317\n",
      "Epoch 473/1000 - Loss: 2.6670475677382344\n",
      "Epoch 474/1000 - Loss: 2.6636036649423116\n",
      "Epoch 475/1000 - Loss: 2.660168643453299\n",
      "Epoch 476/1000 - Loss: 2.6567424803671598\n",
      "Epoch 477/1000 - Loss: 2.6533251528389283\n",
      "Epoch 478/1000 - Loss: 2.6499166380825625\n",
      "Epoch 479/1000 - Loss: 2.6465169133707893\n",
      "Epoch 480/1000 - Loss: 2.6431259560349534\n",
      "Epoch 481/1000 - Loss: 2.639743743464863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482/1000 - Loss: 2.6363702531086446\n",
      "Epoch 483/1000 - Loss: 2.6330054624725854\n",
      "Epoch 484/1000 - Loss: 2.629649349120989\n",
      "Epoch 485/1000 - Loss: 2.6263018906760243\n",
      "Epoch 486/1000 - Loss: 2.6229630648175744\n",
      "Epoch 487/1000 - Loss: 2.6196328492830854\n",
      "Epoch 488/1000 - Loss: 2.6163112218674254\n",
      "Epoch 489/1000 - Loss: 2.6129981604227286\n",
      "Epoch 490/1000 - Loss: 2.609693642858252\n",
      "Epoch 491/1000 - Loss: 2.6063976471402244\n",
      "Epoch 492/1000 - Loss: 2.6031101512917036\n",
      "Epoch 493/1000 - Loss: 2.599831133392427\n",
      "Epoch 494/1000 - Loss: 2.5965605715786637\n",
      "Epoch 495/1000 - Loss: 2.5932984440430737\n",
      "Epoch 496/1000 - Loss: 2.5900447290345565\n",
      "Epoch 497/1000 - Loss: 2.5867994048581098\n",
      "Epoch 498/1000 - Loss: 2.5835624498746843\n",
      "Epoch 499/1000 - Loss: 2.5803338425010387\n",
      "Epoch 500/1000 - Loss: 2.5771135612095923\n",
      "Epoch 501/1000 - Loss: 2.5739015845282887\n",
      "Epoch 502/1000 - Loss: 2.5706978910404477\n",
      "Epoch 503/1000 - Loss: 2.56750245938462\n",
      "Epoch 504/1000 - Loss: 2.5643152682544508\n",
      "Epoch 505/1000 - Loss: 2.5611362963985345\n",
      "Epoch 506/1000 - Loss: 2.5579655226202713\n",
      "Epoch 507/1000 - Loss: 2.5548029257777296\n",
      "Epoch 508/1000 - Loss: 2.551648484783499\n",
      "Epoch 509/1000 - Loss: 2.5485021786045587\n",
      "Epoch 510/1000 - Loss: 2.545363986262128\n",
      "Epoch 511/1000 - Loss: 2.5422338868315313\n",
      "Epoch 512/1000 - Loss: 2.5391118594420576\n",
      "Epoch 513/1000 - Loss: 2.5359978832768215\n",
      "Epoch 514/1000 - Loss: 2.5328919375726224\n",
      "Epoch 515/1000 - Loss: 2.52979400161981\n",
      "Epoch 516/1000 - Loss: 2.526704054762139\n",
      "Epoch 517/1000 - Loss: 2.523622076396642\n",
      "Epoch 518/1000 - Loss: 2.5205480459734804\n",
      "Epoch 519/1000 - Loss: 2.517481942995817\n",
      "Epoch 520/1000 - Loss: 2.514423747019673\n",
      "Epoch 521/1000 - Loss: 2.511373437653795\n",
      "Epoch 522/1000 - Loss: 2.5083309945595156\n",
      "Epoch 523/1000 - Loss: 2.5052963974506226\n",
      "Epoch 524/1000 - Loss: 2.502269626093219\n",
      "Epoch 525/1000 - Loss: 2.49925066030559\n",
      "Epoch 526/1000 - Loss: 2.4962394799580703\n",
      "Epoch 527/1000 - Loss: 2.4932360649729066\n",
      "Epoch 528/1000 - Loss: 2.490240395324124\n",
      "Epoch 529/1000 - Loss: 2.4872524510373952\n",
      "Epoch 530/1000 - Loss: 2.4842722121899055\n",
      "Epoch 531/1000 - Loss: 2.481299658910217\n",
      "Epoch 532/1000 - Loss: 2.4783347713781434\n",
      "Epoch 533/1000 - Loss: 2.4753775298246095\n",
      "Epoch 534/1000 - Loss: 2.472427914531525\n",
      "Epoch 535/1000 - Loss: 2.4694859058316503\n",
      "Epoch 536/1000 - Loss: 2.4665514841084692\n",
      "Epoch 537/1000 - Loss: 2.4636246297960502\n",
      "Epoch 538/1000 - Loss: 2.460705323378925\n",
      "Epoch 539/1000 - Loss: 2.4577935453919517\n",
      "Epoch 540/1000 - Loss: 2.4548892764201895\n",
      "Epoch 541/1000 - Loss: 2.451992497098765\n",
      "Epoch 542/1000 - Loss: 2.449103188112747\n",
      "Epoch 543/1000 - Loss: 2.4462213301970155\n",
      "Epoch 544/1000 - Loss: 2.443346904136134\n",
      "Epoch 545/1000 - Loss: 2.4404798907642196\n",
      "Epoch 546/1000 - Loss: 2.4376202709648185\n",
      "Epoch 547/1000 - Loss: 2.434768025670775\n",
      "Epoch 548/1000 - Loss: 2.431923135864108\n",
      "Epoch 549/1000 - Loss: 2.4290855825758793\n",
      "Epoch 550/1000 - Loss: 2.426255346886071\n",
      "Epoch 551/1000 - Loss: 2.4234324099234605\n",
      "Epoch 552/1000 - Loss: 2.4206167528654894\n",
      "Epoch 553/1000 - Loss: 2.4178083569381426\n",
      "Epoch 554/1000 - Loss: 2.4150072034158203\n",
      "Epoch 555/1000 - Loss: 2.412213273621216\n",
      "Epoch 556/1000 - Loss: 2.4094265489251874\n",
      "Epoch 557/1000 - Loss: 2.4066470107466387\n",
      "Epoch 558/1000 - Loss: 2.40387464055239\n",
      "Epoch 559/1000 - Loss: 2.401109419857057\n",
      "Epoch 560/1000 - Loss: 2.3983513302229302\n",
      "Epoch 561/1000 - Loss: 2.3956003532598458\n",
      "Epoch 562/1000 - Loss: 2.392856470625067\n",
      "Epoch 563/1000 - Loss: 2.3901196640231634\n",
      "Epoch 564/1000 - Loss: 2.3873899152058846\n",
      "Epoch 565/1000 - Loss: 2.384667205972041\n",
      "Epoch 566/1000 - Loss: 2.3819515181673823\n",
      "Epoch 567/1000 - Loss: 2.3792428336844753\n",
      "Epoch 568/1000 - Loss: 2.376541134462586\n",
      "Epoch 569/1000 - Loss: 2.373846402487555\n",
      "Epoch 570/1000 - Loss: 2.3711586197916814\n",
      "Epoch 571/1000 - Loss: 2.3684777684536\n",
      "Epoch 572/1000 - Loss: 2.365803830598162\n",
      "Epoch 573/1000 - Loss: 2.3631367883963192\n",
      "Epoch 574/1000 - Loss: 2.3604766240650012\n",
      "Epoch 575/1000 - Loss: 2.357823319866999\n",
      "Epoch 576/1000 - Loss: 2.355176858110844\n",
      "Epoch 577/1000 - Loss: 2.3525372211506945\n",
      "Epoch 578/1000 - Loss: 2.3499043913862145\n",
      "Epoch 579/1000 - Loss: 2.347278351262456\n",
      "Epoch 580/1000 - Loss: 2.3446590832697467\n",
      "Epoch 581/1000 - Loss: 2.3420465699435664\n",
      "Epoch 582/1000 - Loss: 2.3394407938644366\n",
      "Epoch 583/1000 - Loss: 2.336841737657801\n",
      "Epoch 584/1000 - Loss: 2.334249383993907\n",
      "Epoch 585/1000 - Loss: 2.3316637155876996\n",
      "Epoch 586/1000 - Loss: 2.329084715198696\n",
      "Epoch 587/1000 - Loss: 2.326512365630875\n",
      "Epoch 588/1000 - Loss: 2.323946649732564\n",
      "Epoch 589/1000 - Loss: 2.321387550396321\n",
      "Epoch 590/1000 - Loss: 2.318835050558821\n",
      "Epoch 591/1000 - Loss: 2.316289133200747\n",
      "Epoch 592/1000 - Loss: 2.3137497813466714\n",
      "Epoch 593/1000 - Loss: 2.3112169780649414\n",
      "Epoch 594/1000 - Loss: 2.3086907064675724\n",
      "Epoch 595/1000 - Loss: 2.3061709497101326\n",
      "Epoch 596/1000 - Loss: 2.3036576909916273\n",
      "Epoch 597/1000 - Loss: 2.3011509135543906\n",
      "Epoch 598/1000 - Loss: 2.298650600683975\n",
      "Epoch 599/1000 - Loss: 2.2961567357090336\n",
      "Epoch 600/1000 - Loss: 2.2936693020012164\n",
      "Epoch 601/1000 - Loss: 2.2911882829750545\n",
      "Epoch 602/1000 - Loss: 2.288713662087851\n",
      "Epoch 603/1000 - Loss: 2.286245422839572\n",
      "Epoch 604/1000 - Loss: 2.283783548772733\n",
      "Epoch 605/1000 - Loss: 2.281328023472295\n",
      "Epoch 606/1000 - Loss: 2.278878830565548\n",
      "Epoch 607/1000 - Loss: 2.276435953722007\n",
      "Epoch 608/1000 - Loss: 2.2739993766533004\n",
      "Epoch 609/1000 - Loss: 2.271569083113064\n",
      "Epoch 610/1000 - Loss: 2.26914505689683\n",
      "Epoch 611/1000 - Loss: 2.2667272818419213\n",
      "Epoch 612/1000 - Loss: 2.2643157418273394\n",
      "Epoch 613/1000 - Loss: 2.2619104207736638\n",
      "Epoch 614/1000 - Loss: 2.259511302642939\n",
      "Epoch 615/1000 - Loss: 2.2571183714385694\n",
      "Epoch 616/1000 - Loss: 2.254731611205214\n",
      "Epoch 617/1000 - Loss: 2.252351006028677\n",
      "Epoch 618/1000 - Loss: 2.2499765400358056\n",
      "Epoch 619/1000 - Loss: 2.247608197394379\n",
      "Epoch 620/1000 - Loss: 2.2452459623130085\n",
      "Epoch 621/1000 - Loss: 2.242889819041029\n",
      "Epoch 622/1000 - Loss: 2.2405397518683947\n",
      "Epoch 623/1000 - Loss: 2.238195745125573\n",
      "Epoch 624/1000 - Loss: 2.235857783183442\n",
      "Epoch 625/1000 - Loss: 2.233525850453185\n",
      "Epoch 626/1000 - Loss: 2.231199931386189\n",
      "Epoch 627/1000 - Loss: 2.228880010473936\n",
      "Epoch 628/1000 - Loss: 2.2265660722479046\n",
      "Epoch 629/1000 - Loss: 2.2242581012794647\n",
      "Epoch 630/1000 - Loss: 2.2219560821797733\n",
      "Epoch 631/1000 - Loss: 2.2196599995996764\n",
      "Epoch 632/1000 - Loss: 2.2173698382296\n",
      "Epoch 633/1000 - Loss: 2.215085582799455\n",
      "Epoch 634/1000 - Loss: 2.2128072180785314\n",
      "Epoch 635/1000 - Loss: 2.210534728875395\n",
      "Epoch 636/1000 - Loss: 2.2082681000377913\n",
      "Epoch 637/1000 - Loss: 2.2060073164525402\n",
      "Epoch 638/1000 - Loss: 2.2037523630454365\n",
      "Epoch 639/1000 - Loss: 2.2015032247811503\n",
      "Epoch 640/1000 - Loss: 2.199259886663128\n",
      "Epoch 641/1000 - Loss: 2.1970223337334858\n",
      "Epoch 642/1000 - Loss: 2.1947905510729178\n",
      "Epoch 643/1000 - Loss: 2.1925645238005917\n",
      "Epoch 644/1000 - Loss: 2.190344237074054\n",
      "Epoch 645/1000 - Loss: 2.1881296760891233\n",
      "Epoch 646/1000 - Loss: 2.1859208260798013\n",
      "Epoch 647/1000 - Loss: 2.1837176723181657\n",
      "Epoch 648/1000 - Loss: 2.181520200114277\n",
      "Epoch 649/1000 - Loss: 2.179328394816081\n",
      "Epoch 650/1000 - Loss: 2.1771422418093076\n",
      "Epoch 651/1000 - Loss: 2.1749617265173744\n",
      "Epoch 652/1000 - Loss: 2.1727868344012933\n",
      "Epoch 653/1000 - Loss: 2.1706175509595673\n",
      "Epoch 654/1000 - Loss: 2.168453861728099\n",
      "Epoch 655/1000 - Loss: 2.166295752280092\n",
      "Epoch 656/1000 - Loss: 2.1641432082259535\n",
      "Epoch 657/1000 - Loss: 2.161996215213201\n",
      "Epoch 658/1000 - Loss: 2.1598547589263655\n",
      "Epoch 659/1000 - Loss: 2.1577188250868953\n",
      "Epoch 660/1000 - Loss: 2.1555883994530607\n",
      "Epoch 661/1000 - Loss: 2.1534634678198623\n",
      "Epoch 662/1000 - Loss: 2.1513440160189305\n",
      "Epoch 663/1000 - Loss: 2.149230029918437\n",
      "Epoch 664/1000 - Loss: 2.1471214954229962\n",
      "Epoch 665/1000 - Loss: 2.1450183984735713\n",
      "Epoch 666/1000 - Loss: 2.1429207250473863\n",
      "Epoch 667/1000 - Loss: 2.1408284611578248\n",
      "Epoch 668/1000 - Loss: 2.138741592854342\n",
      "Epoch 669/1000 - Loss: 2.1366601062223674\n",
      "Epoch 670/1000 - Loss: 2.134583987383218\n",
      "Epoch 671/1000 - Loss: 2.1325132224940004\n",
      "Epoch 672/1000 - Loss: 2.130447797747521\n",
      "Epoch 673/1000 - Loss: 2.1283876993721917\n",
      "Epoch 674/1000 - Loss: 2.126332913631941\n",
      "Epoch 675/1000 - Loss: 2.1242834268261213\n",
      "Epoch 676/1000 - Loss: 2.122239225289417\n",
      "Epoch 677/1000 - Loss: 2.120200295391754\n",
      "Epoch 678/1000 - Loss: 2.1181666235382064\n",
      "Epoch 679/1000 - Loss: 2.116138196168912\n",
      "Epoch 680/1000 - Loss: 2.1141149997589745\n",
      "Epoch 681/1000 - Loss: 2.1120970208183776\n",
      "Epoch 682/1000 - Loss: 2.110084245891896\n",
      "Epoch 683/1000 - Loss: 2.1080766615589996\n",
      "Epoch 684/1000 - Loss: 2.106074254433773\n",
      "Epoch 685/1000 - Loss: 2.104077011164819\n",
      "Epoch 686/1000 - Loss: 2.1020849184351715\n",
      "Epoch 687/1000 - Loss: 2.100097962962209\n",
      "Epoch 688/1000 - Loss: 2.098116131497563\n",
      "Epoch 689/1000 - Loss: 2.096139410827032\n",
      "Epoch 690/1000 - Loss: 2.0941677877704903\n",
      "Epoch 691/1000 - Loss: 2.0922012491818043\n",
      "Epoch 692/1000 - Loss: 2.0902397819487426\n",
      "Epoch 693/1000 - Loss: 2.088283372992888\n",
      "Epoch 694/1000 - Loss: 2.08633200926955\n",
      "Epoch 695/1000 - Loss: 2.084385677767681\n",
      "Epoch 696/1000 - Loss: 2.0824443655097853\n",
      "Epoch 697/1000 - Loss: 2.080508059551836\n",
      "Epoch 698/1000 - Loss: 2.0785767469831864\n",
      "Epoch 699/1000 - Loss: 2.076650414926486\n",
      "Epoch 700/1000 - Loss: 2.074729050537591\n",
      "Epoch 701/1000 - Loss: 2.0728126410054837\n",
      "Epoch 702/1000 - Loss: 2.0709011735521847\n",
      "Epoch 703/1000 - Loss: 2.0689946354326643\n",
      "Epoch 704/1000 - Loss: 2.0670930139347643\n",
      "Epoch 705/1000 - Loss: 2.065196296379107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706/1000 - Loss: 2.0633044701190144\n",
      "Epoch 707/1000 - Loss: 2.0614175225404225\n",
      "Epoch 708/1000 - Loss: 2.0595354410617985\n",
      "Epoch 709/1000 - Loss: 2.0576582131340553\n",
      "Epoch 710/1000 - Loss: 2.055785826240466\n",
      "Epoch 711/1000 - Loss: 2.0539182678965875\n",
      "Epoch 712/1000 - Loss: 2.0520555256501685\n",
      "Epoch 713/1000 - Loss: 2.0501975870810725\n",
      "Epoch 714/1000 - Loss: 2.0483444398011916\n",
      "Epoch 715/1000 - Loss: 2.046496071454369\n",
      "Epoch 716/1000 - Loss: 2.0446524697163073\n",
      "Epoch 717/1000 - Loss: 2.0428136222944975\n",
      "Epoch 718/1000 - Loss: 2.040979516928127\n",
      "Epoch 719/1000 - Loss: 2.0391501413880055\n",
      "Epoch 720/1000 - Loss: 2.037325483476479\n",
      "Epoch 721/1000 - Loss: 2.0355055310273507\n",
      "Epoch 722/1000 - Loss: 2.033690271905796\n",
      "Epoch 723/1000 - Loss: 2.031879694008289\n",
      "Epoch 724/1000 - Loss: 2.030073785262515\n",
      "Epoch 725/1000 - Loss: 2.028272533627291\n",
      "Epoch 726/1000 - Loss: 2.02647592709249\n",
      "Epoch 727/1000 - Loss: 2.024683953678954\n",
      "Epoch 728/1000 - Loss: 2.0228966014384215\n",
      "Epoch 729/1000 - Loss: 2.0211138584534427\n",
      "Epoch 730/1000 - Loss: 2.019335712837299\n",
      "Epoch 731/1000 - Loss: 2.01756215273393\n",
      "Epoch 732/1000 - Loss: 2.0157931663178483\n",
      "Epoch 733/1000 - Loss: 2.0140287417940645\n",
      "Epoch 734/1000 - Loss: 2.012268867398006\n",
      "Epoch 735/1000 - Loss: 2.010513531395439\n",
      "Epoch 736/1000 - Loss: 2.0087627220823925\n",
      "Epoch 737/1000 - Loss: 2.007016427785079\n",
      "Epoch 738/1000 - Loss: 2.0052746368598138\n",
      "Epoch 739/1000 - Loss: 2.0035373376929417\n",
      "Epoch 740/1000 - Loss: 2.001804518700757\n",
      "Epoch 741/1000 - Loss: 2.000076168329428\n",
      "Epoch 742/1000 - Loss: 1.9983522750549176\n",
      "Epoch 743/1000 - Loss: 1.99663282738291\n",
      "Epoch 744/1000 - Loss: 1.9949178138487282\n",
      "Epoch 745/1000 - Loss: 1.9932072230172648\n",
      "Epoch 746/1000 - Loss: 1.9915010434829\n",
      "Epoch 747/1000 - Loss: 1.989799263869429\n",
      "Epoch 748/1000 - Loss: 1.9881018728299842\n",
      "Epoch 749/1000 - Loss: 1.9864088590469593\n",
      "Epoch 750/1000 - Loss: 1.984720211231937\n",
      "Epoch 751/1000 - Loss: 1.98303591812561\n",
      "Epoch 752/1000 - Loss: 1.9813559684977078\n",
      "Epoch 753/1000 - Loss: 1.9796803511469203\n",
      "Epoch 754/1000 - Loss: 1.9780090549008267\n",
      "Epoch 755/1000 - Loss: 1.9763420686158157\n",
      "Epoch 756/1000 - Loss: 1.9746793811770162\n",
      "Epoch 757/1000 - Loss: 1.9730209814982202\n",
      "Epoch 758/1000 - Loss: 1.971366858521809\n",
      "Epoch 759/1000 - Loss: 1.9697170012186813\n",
      "Epoch 760/1000 - Loss: 1.968071398588178\n",
      "Epoch 761/1000 - Loss: 1.9664300396580086\n",
      "Epoch 762/1000 - Loss: 1.9647929134841815\n",
      "Epoch 763/1000 - Loss: 1.9631600091509254\n",
      "Epoch 764/1000 - Loss: 1.9615313157706211\n",
      "Epoch 765/1000 - Loss: 1.9599068224837264\n",
      "Epoch 766/1000 - Loss: 1.9582865184587048\n",
      "Epoch 767/1000 - Loss: 1.9566703928919527\n",
      "Epoch 768/1000 - Loss: 1.9550584350077294\n",
      "Epoch 769/1000 - Loss: 1.9534506340580813\n",
      "Epoch 770/1000 - Loss: 1.9518469793227744\n",
      "Epoch 771/1000 - Loss: 1.9502474601092186\n",
      "Epoch 772/1000 - Loss: 1.9486520657524007\n",
      "Epoch 773/1000 - Loss: 1.947060785614811\n",
      "Epoch 774/1000 - Loss: 1.9454736090863722\n",
      "Epoch 775/1000 - Loss: 1.9438905255843688\n",
      "Epoch 776/1000 - Loss: 1.9423115245533777\n",
      "Epoch 777/1000 - Loss: 1.9407365954651965\n",
      "Epoch 778/1000 - Loss: 1.9391657278187735\n",
      "Epoch 779/1000 - Loss: 1.9375989111401386\n",
      "Epoch 780/1000 - Loss: 1.936036134982332\n",
      "Epoch 781/1000 - Loss: 1.9344773889253366\n",
      "Epoch 782/1000 - Loss: 1.9329226625760059\n",
      "Epoch 783/1000 - Loss: 1.9313719455679965\n",
      "Epoch 784/1000 - Loss: 1.9298252275616996\n",
      "Epoch 785/1000 - Loss: 1.9282824982441695\n",
      "Epoch 786/1000 - Loss: 1.9267437473290574\n",
      "Epoch 787/1000 - Loss: 1.9252089645565411\n",
      "Epoch 788/1000 - Loss: 1.9236781396932576\n",
      "Epoch 789/1000 - Loss: 1.9221512625322341\n",
      "Epoch 790/1000 - Loss: 1.9206283228928211\n",
      "Epoch 791/1000 - Loss: 1.919109310620623\n",
      "Epoch 792/1000 - Loss: 1.9175942155874313\n",
      "Epoch 793/1000 - Loss: 1.9160830276911582\n",
      "Epoch 794/1000 - Loss: 1.9145757368557659\n",
      "Epoch 795/1000 - Loss: 1.9130723330312023\n",
      "Epoch 796/1000 - Loss: 1.9115728061933348\n",
      "Epoch 797/1000 - Loss: 1.9100771463438795\n",
      "Epoch 798/1000 - Loss: 1.9085853435103375\n",
      "Epoch 799/1000 - Loss: 1.9070973877459285\n",
      "Epoch 800/1000 - Loss: 1.9056132691295231\n",
      "Epoch 801/1000 - Loss: 1.9041329777655778\n",
      "Epoch 802/1000 - Loss: 1.9026565037840681\n",
      "Epoch 803/1000 - Loss: 1.9011838373404233\n",
      "Epoch 804/1000 - Loss: 1.8997149686154609\n",
      "Epoch 805/1000 - Loss: 1.8982498878153198\n",
      "Epoch 806/1000 - Loss: 1.8967885851713975\n",
      "Epoch 807/1000 - Loss: 1.895331050940283\n",
      "Epoch 808/1000 - Loss: 1.893877275403692\n",
      "Epoch 809/1000 - Loss: 1.8924272488684035\n",
      "Epoch 810/1000 - Loss: 1.8909809616661928\n",
      "Epoch 811/1000 - Loss: 1.8895384041537693\n",
      "Epoch 812/1000 - Loss: 1.888099566712711\n",
      "Epoch 813/1000 - Loss: 1.8866644397494008\n",
      "Epoch 814/1000 - Loss: 1.8852330136949618\n",
      "Epoch 815/1000 - Loss: 1.8838052790051958\n",
      "Epoch 816/1000 - Loss: 1.8823812261605155\n",
      "Epoch 817/1000 - Loss: 1.8809608456658853\n",
      "Epoch 818/1000 - Loss: 1.8795441280507557\n",
      "Epoch 819/1000 - Loss: 1.8781310638689992\n",
      "Epoch 820/1000 - Loss: 1.8767216436988516\n",
      "Epoch 821/1000 - Loss: 1.8753158581428444\n",
      "Epoch 822/1000 - Loss: 1.8739136978277444\n",
      "Epoch 823/1000 - Loss: 1.8725151534044893\n",
      "Epoch 824/1000 - Loss: 1.8711202155481301\n",
      "Epoch 825/1000 - Loss: 1.8697288749577634\n",
      "Epoch 826/1000 - Loss: 1.868341122356472\n",
      "Epoch 827/1000 - Loss: 1.8669569484912636\n",
      "Epoch 828/1000 - Loss: 1.8655763441330078\n",
      "Epoch 829/1000 - Loss: 1.8641993000763746\n",
      "Epoch 830/1000 - Loss: 1.8628258071397732\n",
      "Epoch 831/1000 - Loss: 1.8614558561652925\n",
      "Epoch 832/1000 - Loss: 1.8600894380186384\n",
      "Epoch 833/1000 - Loss: 1.8587265435890705\n",
      "Epoch 834/1000 - Loss: 1.8573671637893463\n",
      "Epoch 835/1000 - Loss: 1.8560112895556584\n",
      "Epoch 836/1000 - Loss: 1.8546589118475725\n",
      "Epoch 837/1000 - Loss: 1.8533100216479692\n",
      "Epoch 838/1000 - Loss: 1.8519646099629834\n",
      "Epoch 839/1000 - Loss: 1.8506226678219435\n",
      "Epoch 840/1000 - Loss: 1.849284186277312\n",
      "Epoch 841/1000 - Loss: 1.8479491564046266\n",
      "Epoch 842/1000 - Loss: 1.8466175693024407\n",
      "Epoch 843/1000 - Loss: 1.8452894160922622\n",
      "Epoch 844/1000 - Loss: 1.8439646879184957\n",
      "Epoch 845/1000 - Loss: 1.8426433759483842\n",
      "Epoch 846/1000 - Loss: 1.8413254713719491\n",
      "Epoch 847/1000 - Loss: 1.840010965401931\n",
      "Epoch 848/1000 - Loss: 1.8386998492737328\n",
      "Epoch 849/1000 - Loss: 1.8373921142453589\n",
      "Epoch 850/1000 - Loss: 1.8360877515973593\n",
      "Epoch 851/1000 - Loss: 1.8347867526327697\n",
      "Epoch 852/1000 - Loss: 1.8334891086770555\n",
      "Epoch 853/1000 - Loss: 1.8321948110780515\n",
      "Epoch 854/1000 - Loss: 1.8309038512059057\n",
      "Epoch 855/1000 - Loss: 1.8296162204530213\n",
      "Epoch 856/1000 - Loss: 1.8283319102339997\n",
      "Epoch 857/1000 - Loss: 1.8270509119855831\n",
      "Epoch 858/1000 - Loss: 1.8257732171665975\n",
      "Epoch 859/1000 - Loss: 1.8244988172578942\n",
      "Epoch 860/1000 - Loss: 1.8232277037622961\n",
      "Epoch 861/1000 - Loss: 1.8219598682045386\n",
      "Epoch 862/1000 - Loss: 1.8206953021312138\n",
      "Epoch 863/1000 - Loss: 1.819433997110714\n",
      "Epoch 864/1000 - Loss: 1.8181759447331753\n",
      "Epoch 865/1000 - Loss: 1.8169211366104225\n",
      "Epoch 866/1000 - Loss: 1.8156695643759126\n",
      "Epoch 867/1000 - Loss: 1.814421219684678\n",
      "Epoch 868/1000 - Loss: 1.8131760942132729\n",
      "Epoch 869/1000 - Loss: 1.8119341796597153\n",
      "Epoch 870/1000 - Loss: 1.8106954677434346\n",
      "Epoch 871/1000 - Loss: 1.8094599502052136\n",
      "Epoch 872/1000 - Loss: 1.808227618807135\n",
      "Epoch 873/1000 - Loss: 1.8069984653325262\n",
      "Epoch 874/1000 - Loss: 1.8057724815859038\n",
      "Epoch 875/1000 - Loss: 1.804549659392921\n",
      "Epoch 876/1000 - Loss: 1.8033299906003102\n",
      "Epoch 877/1000 - Loss: 1.80211346707583\n",
      "Epoch 878/1000 - Loss: 1.8009000807082136\n",
      "Epoch 879/1000 - Loss: 1.7996898234071086\n",
      "Epoch 880/1000 - Loss: 1.798482687103029\n",
      "Epoch 881/1000 - Loss: 1.7972786637472982\n",
      "Epoch 882/1000 - Loss: 1.7960777453119967\n",
      "Epoch 883/1000 - Loss: 1.7948799237899071\n",
      "Epoch 884/1000 - Loss: 1.793685191194463\n",
      "Epoch 885/1000 - Loss: 1.7924935395596928\n",
      "Epoch 886/1000 - Loss: 1.7913049609401697\n",
      "Epoch 887/1000 - Loss: 1.7901194474109563\n",
      "Epoch 888/1000 - Loss: 1.7889369910675532\n",
      "Epoch 889/1000 - Loss: 1.7877575840258446\n",
      "Epoch 890/1000 - Loss: 1.7865812184220489\n",
      "Epoch 891/1000 - Loss: 1.7854078864126621\n",
      "Epoch 892/1000 - Loss: 1.784237580174409\n",
      "Epoch 893/1000 - Loss: 1.7830702919041894\n",
      "Epoch 894/1000 - Loss: 1.7819060138190261\n",
      "Epoch 895/1000 - Loss: 1.7807447381560133\n",
      "Epoch 896/1000 - Loss: 1.779586457172266\n",
      "Epoch 897/1000 - Loss: 1.778431163144865\n",
      "Epoch 898/1000 - Loss: 1.777278848370809\n",
      "Epoch 899/1000 - Loss: 1.7761295051669612\n",
      "Epoch 900/1000 - Loss: 1.7749831258699997\n",
      "Epoch 901/1000 - Loss: 1.7738397028363637\n",
      "Epoch 902/1000 - Loss: 1.7726992284422067\n",
      "Epoch 903/1000 - Loss: 1.7715616950833404\n",
      "Epoch 904/1000 - Loss: 1.7704270951751888\n",
      "Epoch 905/1000 - Loss: 1.7692954211527343\n",
      "Epoch 906/1000 - Loss: 1.7681666654704695\n",
      "Epoch 907/1000 - Loss: 1.7670408206023456\n",
      "Epoch 908/1000 - Loss: 1.765917879041723\n",
      "Epoch 909/1000 - Loss: 1.76479783330132\n",
      "Epoch 910/1000 - Loss: 1.7636806759131642\n",
      "Epoch 911/1000 - Loss: 1.762566399428542\n",
      "Epoch 912/1000 - Loss: 1.7614549964179496\n",
      "Epoch 913/1000 - Loss: 1.7603464594710423\n",
      "Epoch 914/1000 - Loss: 1.759240781196586\n",
      "Epoch 915/1000 - Loss: 1.7581379542224083\n",
      "Epoch 916/1000 - Loss: 1.757037971195348\n",
      "Epoch 917/1000 - Loss: 1.7559408247812074\n",
      "Epoch 918/1000 - Loss: 1.7548465076647024\n",
      "Epoch 919/1000 - Loss: 1.753755012549414\n",
      "Epoch 920/1000 - Loss: 1.752666332157741\n",
      "Epoch 921/1000 - Loss: 1.7515804592308482\n",
      "Epoch 922/1000 - Loss: 1.7504973865286217\n",
      "Epoch 923/1000 - Loss: 1.749417106829618\n",
      "Epoch 924/1000 - Loss: 1.7483396129310187\n",
      "Epoch 925/1000 - Loss: 1.7472648976485776\n",
      "Epoch 926/1000 - Loss: 1.7461929538165781\n",
      "Epoch 927/1000 - Loss: 1.7451237742877828\n",
      "Epoch 928/1000 - Loss: 1.744057351933385\n",
      "Epoch 929/1000 - Loss: 1.742993679642964\n",
      "Epoch 930/1000 - Loss: 1.7419327503244346\n",
      "Epoch 931/1000 - Loss: 1.740874556904002\n",
      "Epoch 932/1000 - Loss: 1.7398190923261136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 933/1000 - Loss: 1.7387663495534127\n",
      "Epoch 934/1000 - Loss: 1.7377163215666893\n",
      "Epoch 935/1000 - Loss: 1.7366690013648385\n",
      "Epoch 936/1000 - Loss: 1.7356243819648074\n",
      "Epoch 937/1000 - Loss: 1.7345824564015524\n",
      "Epoch 938/1000 - Loss: 1.733543217727992\n",
      "Epoch 939/1000 - Loss: 1.7325066590149607\n",
      "Epoch 940/1000 - Loss: 1.7314727733511628\n",
      "Epoch 941/1000 - Loss: 1.730441553843124\n",
      "Epoch 942/1000 - Loss: 1.7294129936151503\n",
      "Epoch 943/1000 - Loss: 1.7283870858092771\n",
      "Epoch 944/1000 - Loss: 1.7273638235852267\n",
      "Epoch 945/1000 - Loss: 1.7263432001203614\n",
      "Epoch 946/1000 - Loss: 1.7253252086096382\n",
      "Epoch 947/1000 - Loss: 1.724309842265564\n",
      "Epoch 948/1000 - Loss: 1.7232970943181485\n",
      "Epoch 949/1000 - Loss: 1.722286958014862\n",
      "Epoch 950/1000 - Loss: 1.7212794266205875\n",
      "Epoch 951/1000 - Loss: 1.7202744934175773\n",
      "Epoch 952/1000 - Loss: 1.7192721517054097\n",
      "Epoch 953/1000 - Loss: 1.7182723948009397\n",
      "Epoch 954/1000 - Loss: 1.7172752160382596\n",
      "Epoch 955/1000 - Loss: 1.7162806087686509\n",
      "Epoch 956/1000 - Loss: 1.7152885663605422\n",
      "Epoch 957/1000 - Loss: 1.7142990821994635\n",
      "Epoch 958/1000 - Loss: 1.7133121496880033\n",
      "Epoch 959/1000 - Loss: 1.712327762245764\n",
      "Epoch 960/1000 - Loss: 1.7113459133093172\n",
      "Epoch 961/1000 - Loss: 1.710366596332162\n",
      "Epoch 962/1000 - Loss: 1.7093898047846792\n",
      "Epoch 963/1000 - Loss: 1.7084155321540895\n",
      "Epoch 964/1000 - Loss: 1.7074437719444078\n",
      "Epoch 965/1000 - Loss: 1.7064745176764036\n",
      "Epoch 966/1000 - Loss: 1.705507762887554\n",
      "Epoch 967/1000 - Loss: 1.7045435011320016\n",
      "Epoch 968/1000 - Loss: 1.7035817259805146\n",
      "Epoch 969/1000 - Loss: 1.7026224310204388\n",
      "Epoch 970/1000 - Loss: 1.7016656098556584\n",
      "Epoch 971/1000 - Loss: 1.7007112561065532\n",
      "Epoch 972/1000 - Loss: 1.6997593634099541\n",
      "Epoch 973/1000 - Loss: 1.6988099254191025\n",
      "Epoch 974/1000 - Loss: 1.6978629358036075\n",
      "Epoch 975/1000 - Loss: 1.6969183882494026\n",
      "Epoch 976/1000 - Loss: 1.6959762764587052\n",
      "Epoch 977/1000 - Loss: 1.695036594149974\n",
      "Epoch 978/1000 - Loss: 1.694099335057867\n",
      "Epoch 979/1000 - Loss: 1.6931644929331995\n",
      "Epoch 980/1000 - Loss: 1.6922320615429025\n",
      "Epoch 981/1000 - Loss: 1.691302034669982\n",
      "Epoch 982/1000 - Loss: 1.690374406113477\n",
      "Epoch 983/1000 - Loss: 1.6894491696884166\n",
      "Epoch 984/1000 - Loss: 1.6885263192257818\n",
      "Epoch 985/1000 - Loss: 1.6876058485724619\n",
      "Epoch 986/1000 - Loss: 1.686687751591215\n",
      "Epoch 987/1000 - Loss: 1.6857720221606256\n",
      "Epoch 988/1000 - Loss: 1.6848586541750652\n",
      "Epoch 989/1000 - Loss: 1.6839476415446502\n",
      "Epoch 990/1000 - Loss: 1.683038978195203\n",
      "Epoch 991/1000 - Loss: 1.6821326580682108\n",
      "Epoch 992/1000 - Loss: 1.6812286751207832\n",
      "Epoch 993/1000 - Loss: 1.6803270233256158\n",
      "Epoch 994/1000 - Loss: 1.6794276966709467\n",
      "Epoch 995/1000 - Loss: 1.6785306891605185\n",
      "Epoch 996/1000 - Loss: 1.6776359948135353\n",
      "Epoch 997/1000 - Loss: 1.6767436076646276\n",
      "Epoch 998/1000 - Loss: 1.6758535217638089\n",
      "Epoch 999/1000 - Loss: 1.6749657311764357\n",
      "Epoch 1000/1000 - Loss: 1.6740802299831716\n",
      "y=0.6275275639871741*X0 + 0.6932322294739053*X1 + 0.6280708378208216*X2 + 0.6164787109742585*X3 + 0.24430084851292205*X4 + 0.6168937786086863*X5 + 0.6930326517006337*X6 + 0.34644447294615793*X7 + 2.389249686392048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201580.46422983805"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Scale the data\n",
    "X_train= (X_train- np.mean(X_train))/np.std(X_train)\n",
    "X_test= (X_test- np.mean(X_train))/np.std(X_train)\n",
    "\n",
    "\n",
    "#Initialize the Linear Regression model\n",
    "lr = LinearRegression(lr=0.001, iterations=1000, alpha=0, lamda=0)\n",
    "\n",
    "#Fit the model to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the test data\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "#Calculate the Mean Squared Error\n",
    "mse = np.mean((y_test-y_pred)**2)\n",
    "\n",
    "print(f'Mean Squared Error is: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
