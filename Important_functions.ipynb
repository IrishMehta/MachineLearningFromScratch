{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed47d0a-5d80-45b2-804a-6c0ee1c5cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1b836-51c7-4843-9b2a-180f9bd7b776",
   "metadata": {},
   "source": [
    "# Implementation of softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3c346df-e20c-4cab-ad77-e9acc793b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated sequence: [111 150 101 126 134]\n",
      "The modified sequence: [-39   0 -49 -24 -16]\n",
      "e^x = [1.15482242e-17 1.00000000e+00 5.24288566e-22 3.77513454e-11\n",
      " 1.12535175e-07]\n",
      "Sum of all e^x = 1.000000112572926\n",
      "Softmax output: [1.15482229e-17 9.99999887e-01 5.24288507e-22 3.77513412e-11\n",
      " 1.12535162e-07]\n"
     ]
    }
   ],
   "source": [
    "#generate a list of random numbers of 5 elements\n",
    "elem_list= np.array([random.randint(100,150) for i in range(5)])\n",
    "print(f'The generated sequence: {elem_list}')\n",
    "\n",
    "#To avoid expoding exponents, we will use a bit of mathematical manipulation\n",
    "logits= elem_list - np.max(elem_list)\n",
    "print(f'The modified sequence: {logits}')\n",
    "\n",
    "# This would get all elements to (min-max(elem_list)) to 0\n",
    "#Now we calculate the exponents\n",
    "numerator= np.exp(logits)\n",
    "denominator= sum(np.exp(logits))\n",
    "print(f'e^x = {numerator}')\n",
    "print(f'Sum of all e^x = {denominator}')\n",
    "\n",
    "output_of_softmax= numerator/denominator\n",
    "print(f'Softmax output: {output_of_softmax}')\n",
    "\n",
    "\n",
    "# Couple of theory pointers-\n",
    "# 1) Subtracting max gives the same output as not subtracting, as the -max(elem_list) power gets cut out with each other when we consider \n",
    "# numerator and denominator together. I.e e^x-c is there in both num and denom, so -c gets cut out \n",
    "\n",
    "# 2) The reason why we take exponents is so that we assign a unique value to each element. Compared to a simple subtraction equation/division, exponents would evenly spread the distribution, and numbers close to the extremes are assigned a probability very close to 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81840e2f-e153-43af-83fe-07690e4ea323",
   "metadata": {},
   "source": [
    "# Implementation of Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "967e0971-0b88-4014-a569-d632457d5a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input List: [-10000   -100    100  10000]\n",
      "The output of using a single formula: [0.00000000e+00 3.72007598e-44 1.00000000e+00 1.00000000e+00]\n",
      "Time taken for single formula: 0.000128 seconds\n",
      "\n",
      "Using numpy.where function for different formula for +/-ve elements: [0.00000000e+00 3.72007598e-44 1.00000000e+00 1.00000000e+00]\n",
      "Time taken for numpy.where: 0.000134 seconds\n",
      "\n",
      "Using split formula using if-else function: [0.00000000e+00 3.72007598e-44 1.00000000e+00 1.00000000e+00]\n",
      "Time taken for Pythonic if-else: 0.000121 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3212378/3509823871.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  exponents = np.exp(-elem_list)\n",
      "/tmp/ipykernel_3212378/3509823871.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(elem_list) / (1 + np.exp(elem_list)),\n",
      "/tmp/ipykernel_3212378/3509823871.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  np.exp(elem_list) / (1 + np.exp(elem_list)),\n",
      "/tmp/ipykernel_3212378/3509823871.py:26: RuntimeWarning: overflow encountered in exp\n",
      "  1 / (1 + np.exp(-elem_list))\n"
     ]
    }
   ],
   "source": [
    "# According to knowledge:\n",
    "# Sigmoid: 1/1+e^(-x)\n",
    "\n",
    "# Input list\n",
    "elem_list = np.array([-10000, -100, 100, 10000])\n",
    "print(f'Input List: {elem_list}')\n",
    "\n",
    "# Method 1: Single formula (unstable)\n",
    "start_time = time.time()\n",
    "exponents = np.exp(-elem_list)\n",
    "sigmoid_values = 1 / (1 + exponents)\n",
    "end_time = time.time()\n",
    "print(f'The output of using a single formula: {sigmoid_values}')\n",
    "print(f'Time taken for single formula: {end_time - start_time:.6f} seconds\\n')\n",
    "\n",
    "# Resolving the overflow-\n",
    "# When x is -infinity, e^-x becomes too large and 1+ e^-x is also large. Hence, when we divide 1 by this large number, it underflows\n",
    "# SO instead of this form of the equation, we use something which will not underflow or overflow, i.e. e^x/(1+e^x), which is just the expanded form of the original equation, but\n",
    "# in this, when x tends to -infinity, numerator will underflow, but it is not a problem as denominator compensates for it\n",
    "# Insane- even this will throw an error. That is because NumPy's where function does not work like an if else condition, it will calculate for both and then pick the one based condition\n",
    "\n",
    "# Method 2: Using numpy.where (partially stable)\n",
    "start_time = time.time()\n",
    "sigmoid_values = np.where(\n",
    "    elem_list < 0,\n",
    "    np.exp(elem_list) / (1 + np.exp(elem_list)),\n",
    "    1 / (1 + np.exp(-elem_list))\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f'Using numpy.where function for different formula for +/-ve elements: {sigmoid_values}')\n",
    "print(f'Time taken for numpy.where: {end_time - start_time:.6f} seconds\\n')\n",
    "\n",
    "# Method 3: Using Pythonic if-else (stable)\n",
    "start_time = time.time()\n",
    "sigmoid_values = []\n",
    "for i in elem_list:\n",
    "    if i < 0:\n",
    "        sigmoid_values.append(np.exp(i) / (1 + np.exp(i)))\n",
    "    else:\n",
    "        sigmoid_values.append(1 / (1 + np.exp(-i)))\n",
    "sigmoid_values = np.array(sigmoid_values)\n",
    "end_time = time.time()\n",
    "print(f'Using split formula using if-else function: {sigmoid_values}')\n",
    "print(f'Time taken for Pythonic if-else: {end_time - start_time:.6f} seconds\\n')\n",
    "\n",
    "# Note:\n",
    "# 1) Sigmoid always tends to 0 or 1 but never touches\n",
    "# 2) Sigmoid is softmax function for 2 classes only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e77cc-ca0d-4a81-887d-22db554289b7",
   "metadata": {},
   "source": [
    "# Implementation of Min max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35f8df1-9c94-4007-ae14-41ac50fa0e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original list is: [589 292 425 942 408]\n",
      "The scaled list is: [0.457 0.    0.205 1.    0.178]\n"
     ]
    }
   ],
   "source": [
    "# The goal is to bring a set of elements to 0 to 1 range. Intuitively, we want to bring the lowest element to 0 and biggest element to 1\n",
    "# So we subtract all elements by the minimum first, which will ensure the lowest element goes to 0. Then, to ensure the largest element goest to 1, we divide this\n",
    "# numerator with max-min, as if x is the max element, to result in 1, we need to divide x - min by max-min\n",
    "\n",
    "#Generating a random list of 5 elements\n",
    "elem_list = np.array([random.randint(100,1000) for _ in range(5)])\n",
    "print(f'The original list is: {elem_list}')\n",
    "\n",
    "#Storing the min value so that we don't do double calculation\n",
    "list_min= np.min(elem_list)\n",
    "\n",
    "# Plugging into the formula\n",
    "scaled_list= (elem_list- list_min)/(np.max(elem_list)-list_min)\n",
    "\n",
    "print(f'The scaled list is: {np.round(scaled_list,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71708d70-2698-4479-930d-fb7072340d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1 contents:\n",
      " [[[[[0.9521847  0.27736915]\n",
      "    [0.02765174 0.30297017]]]]]\n",
      "Array 2 contents:\n",
      " [[[[[0.79863372 0.31426173]\n",
      "    [0.80371021 0.96800607]]]]]\n",
      "The euclidean distance between these n-dim arrays is: 1.0341559216259615\n"
     ]
    }
   ],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "# Euclidean distance is the most basic distance measure (after manhattan distance) that one could implement. This should be super quick\n",
    "\n",
    "# Formula: sqrt( x distance squared + y distance squared)\n",
    "# Oh but wait, for n dimensions, it would be (sum(xi-yi)^2)^0.5\n",
    "\n",
    "# generate a 4 dimensional array\n",
    "random_4d_array_1 = np.random.rand(1,1,1,2,2)\n",
    "random_4d_array_2= np.random.rand(1,1,1,2,2)\n",
    "\n",
    "print(\"Array 1 contents:\\n\", random_4d_array_1)\n",
    "print(\"Array 2 contents:\\n\", random_4d_array_2)\n",
    "\n",
    "euclidean_dist= np.sqrt(np.sum(np.square(random_4d_array_1-random_4d_array_2)))\n",
    "print(f'The euclidean distance between these n-dim arrays is: {euclidean_dist}')\n",
    "\n",
    "# print(arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb0597-88c7-40ed-a3a6-0d2fe66aba31",
   "metadata": {},
   "source": [
    "# Implementation of Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b00100d-fe19-4527-8437-4cd53f7da146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1 contents:\n",
      " [[0.24866596 0.89507646 0.6725381 ]\n",
      " [0.51994643 0.32368479 0.9081165 ]\n",
      " [0.29786276 0.11547373 0.05686165]\n",
      " [0.79839683 0.87496723 0.7025378 ]\n",
      " [0.46081942 0.84149323 0.37804283]]\n",
      "The covariance between all the features: [[0.0472313  0.02315534 0.03179219]\n",
      " [0.02315534 0.13289947 0.04422086]\n",
      " [0.03179219 0.04422086 0.10977048]]\n",
      "The mahalanobis distance is: 2.495460926259866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# THis is a fancy distance taught in data mining class and used mainly to accoud for correlation plust different scales between two variables. Let's try to implement\n",
    "# it using numpy\n",
    "\n",
    "# Some theory- Mahalanobis distance essentially measures the distance between two points relative to a dataset. Rather than relying on Euclidean estimates, \n",
    "# which can fail in a lot of cases, this is a more customized distance which helps to determine how far data points are based on correlation and scale of features\n",
    "\n",
    "# Formula: Square root of (x-y (transpose) . inverse of covariance . x-y)\n",
    "\n",
    "# generating n random 3d arrays:\n",
    "n=5\n",
    "random_4d_array = np.random.rand(n,3)\n",
    "\n",
    "print(\"Array 1 contents:\\n\", random_4d_array)\n",
    "\n",
    "# calculating covariance between the arrays (we do row_var=False as we want to consider our data set in the column format as covariance is between columns)\n",
    "covariance= np.cov(random_4d_array, rowvar=False)\n",
    "print(f'The covariance between all the features: {covariance}')\n",
    "\n",
    "# Let's say we want to find distance between point 1 and point 2\n",
    "point1 = random_4d_array[0]\n",
    "point2 = random_4d_array[1]\n",
    "diff_bw_input_arr= point1-point2\n",
    "\n",
    "# Multiple learnings-\n",
    "# In python: @ is for dot product\n",
    "# np.linalg.inv is required for inverting a matrix since the covariance is not a fixed number\n",
    "# There are two ways to transpose, .T or np.transpose()\n",
    "\n",
    "mahalanobis_dist= np.sqrt(diff_bw_input_arr @ np.linalg.inv(covariance) @ diff_bw_input_arr)\n",
    "print(f'The mahalanobis distance is: {mahalanobis_dist}')\n",
    "\n",
    "\n",
    "\n",
    "# Use cases:\n",
    "# outlier detection between financial transactions\n",
    "# outlier detection in manufacturing (sensor data)\n",
    "# outlier detection in patient health data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01462b8b-7bba-4422-8bd9-6d635382ed7a",
   "metadata": {},
   "source": [
    "# Implementation of Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b64ae84-d18a-43aa-be8b-3e9f9178e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.995732273553991\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss fits well with classification problems and deep learning networks, as they couple with softmax outputs\n",
    "# The reason is that cross entropy penalizes wrong predictions significantly (higher the different between actual and predicted output, higher the amplification \n",
    "# in the loss function. This amplification happens because of the log function in the equation. This is also why cross entropy allows faster training, as amplified gradients speed up the process\n",
    "\n",
    "# Formula: - sum( actual output value (y) x log predicted probability (y_hat)\n",
    "\n",
    "# Generating 2 arrays, one with the actual output, one with the predicted output\n",
    "actual_output= [0, 0, 1, 0]\n",
    "predicted_output = [0.8, 0.1, 0.05, 0.05]\n",
    "\n",
    "# One drawback of the normal formula is that when probability tends to 0, the log function can go really low and the loss value can shoot up a lot\n",
    "# So we keep a cap\n",
    "predicted_output= np.clip(predicted_output, a_max= 1, a_min=1e-12)\n",
    "\n",
    "# Loss calculation\n",
    "cross_entropy_loss = -(np.sum(np.array(actual_output) * np.log(np.array(predicted_output))))\n",
    "print(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fb7d5-53a4-445c-a2f0-196b09b7553f",
   "metadata": {},
   "source": [
    "# Gradient Descent/Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "452aca4f-bcc4-4a1a-9af4-cb2d2e27c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: \n",
      "[[0.67298596 0.38498319]\n",
      " [0.10279585 0.25651577]\n",
      " [0.49949157 0.52126978]]\n",
      "Old weights: [[0.98223625]]\n",
      "Mean Squared Error: 0.11951138274323655\n",
      "The gradient w.r.t W: 0.9221518240504133\n",
      "The gradient w.r.t bias: 1.3797019884386448\n",
      "New weights: [[0.9813141]]\n",
      "New bias: 0.19862029801156136\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent is relatively simple but super powerful\n",
    "# However, implementation gets complex one we have multiple features \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Generating a random dataset (3 samples, 1 feature)\n",
    "data = np.random.rand(3,2)\n",
    "print(f'Original Data: \\n{data}')\n",
    "\n",
    "x = data[:, 0].reshape(-1, 1)  # Shape (3,1)\n",
    "y = data[:, 1]  # Shape (3,)\n",
    "\n",
    "# weight and bias\n",
    "W = np.random.rand(1, 1) \n",
    "bias = 0.2\n",
    "print(f'Old weights: {W}')\n",
    "\n",
    "# Predicted output\n",
    "y_pred = (x @ W) + bias\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = np.mean(np.square(y - y_pred))\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Compute gradient of MSE w.r.t. W\n",
    "gradient_W = -(2/len(x)) * np.sum(x * (y - y_pred))  # Correct formula\n",
    "gradient_b = -(2/len(x)) * np.sum(y - y_pred)  # Gradient for bias\n",
    "\n",
    "print(f'The gradient w.r.t W: {gradient_W}')\n",
    "print(f'The gradient w.r.t bias: {gradient_b}')\n",
    "\n",
    "# Gradient Descent Step\n",
    "learning_rate = 0.001\n",
    "W_new = W - learning_rate * gradient_W  # Update weight\n",
    "bias_new = bias - learning_rate * gradient_b  # Update bias\n",
    "\n",
    "print(f'New weights: {W_new}')\n",
    "print(f'New bias: {bias_new}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7139ac8-e859-471e-a369-113a039f6fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058bf3fc-7312-4b0f-9c75-9fc07fd9e4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce8f72-18c1-41f8-b89d-4213ae6dd222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce8eb0-52b2-4c6c-8d6a-08e5a8330e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034f334-de78-4ce8-aeae-efc203d7905e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8b3fe-594c-4a76-a996-655b8e40aab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e6bde-e83e-44cf-abaf-6f3a184eb275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
