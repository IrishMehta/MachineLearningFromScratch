{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18c9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import e\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from numpy import random\n",
    "\n",
    "# Load the dataset\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "\n",
    "def preprocess_titanic_data(df):\n",
    "\n",
    "    df = df.drop(['embarked', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], axis=1)\n",
    "\n",
    "    df['age'].fillna(df['age'].median(), inplace=True)\n",
    "    \n",
    "    labelencoder = LabelEncoder()\n",
    "    df['sex'] = labelencoder.fit_transform(df['sex'])\n",
    "    df['class'] = labelencoder.fit_transform(df['class'])\n",
    "    \n",
    "\n",
    "    X = df.drop('survived', axis=1)\n",
    "    y = df['survived']\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_titanic_data(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275cddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1283938  -1.19870739 -1.36619563  0.53180196  1.17908404 -0.08609342\n",
      " -0.75935275 -0.51763074]\n",
      "Epoch 0, Loss: 0.7035341736105\n",
      "Epoch 100, Loss: 0.6524156614404523\n",
      "Epoch 200, Loss: 0.6125959818403729\n",
      "Epoch 300, Loss: 0.5815060369338859\n",
      "Epoch 400, Loss: 0.5571096745897782\n",
      "Epoch 500, Loss: 0.537865657244996\n",
      "Epoch 600, Loss: 0.5225744377047781\n",
      "Epoch 700, Loss: 0.5102903592122138\n",
      "Epoch 800, Loss: 0.5002988071473173\n",
      "Epoch 900, Loss: 0.4920850378131642\n",
      "Final weights: [-0.59324435 -0.87968143 -1.38884711 -0.1237624   0.24395693 -0.20334404\n",
      " -0.37459791 -0.19860478]\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(logit):\n",
    "    return 1 / (1 + np.exp(-logit))\n",
    "\n",
    "# Loss function: binary crossentropy\n",
    "def loss_calculator(y_pred, y_act):\n",
    "    # clipping because otherwise the log function can go to infinity\n",
    "    y_pred= np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_act * np.log(y_pred) + (1 - y_act) * np.log(1 - y_pred))\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(X, y_pred, y_act, weights, learning_rate=0.01):\n",
    "    m = len(y_act)\n",
    "    \n",
    "    # Transposing X so that the dot product is feasible\n",
    "    gradient = np.dot(X.T, (y_pred - y_act)) / m\n",
    "    weights -= learning_rate * gradient\n",
    "    return weights\n",
    "\n",
    "# Logistic Regression\n",
    "def logistic_regression(X, y_act, epochs=1000, learning_rate=0.01):\n",
    "    \n",
    "    # adding the intercept term to the dataset so that its easier to take the dot product \n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    # Initialize weights\n",
    "    weights = random.randn(X.shape[1])\n",
    "    print(weights)\n",
    "    for i in range(epochs):\n",
    "        # Compute logit\n",
    "        logit = np.dot(X, weights)\n",
    "\n",
    "        y_pred = sigmoid(logit)\n",
    "        \n",
    "        loss = loss_calculator(y_pred, y_act)\n",
    "        \n",
    "        weights = gradient_descent(X, y_pred, y_act, weights, learning_rate)\n",
    "        \n",
    "        # print loss every 100 epochs\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}, Loss: {loss}\")\n",
    "            \n",
    "    return weights\n",
    "\n",
    "def predict(weights, X_test, y_test):\n",
    "    test= np.hstack((np.ones((X_test.shape[0],1)), X_test))\n",
    "    y_prob= sigmoid(np.dot(test,weights))\n",
    "    output_list=[0 for i in range(len(X_test))]\n",
    "    for index, i in enumerate(y_prob):\n",
    "        if i>0.5:\n",
    "            output_list[index]=1\n",
    "        else:\n",
    "            output_list[index]=0\n",
    "    return output_list\n",
    "    \n",
    "\n",
    "# Run logistic regression\n",
    "weights = logistic_regression(X_train, y_train)\n",
    "print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508de53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.8212290502793296\n",
      "Precision: 0.7625\n",
      "Recall: 0.8243243243243243\n",
      "F1 Score: 0.7922077922077922\n",
      "True Positives: 61\n",
      "True Negatives: 86\n",
      "False Positives: 19\n",
      "False Negatives: 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred)\n",
    "    metrics['F1 Score'] = f1_score(y_true, y_pred)\n",
    "    \n",
    "    metrics['True Positives']= confusion_matrix(y_true,y_pred)[1][1]\n",
    "    metrics['True Negatives']= confusion_matrix(y_true,y_pred)[0][0]\n",
    "    metrics['False Positives']= confusion_matrix(y_true,y_pred)[0][1]\n",
    "    metrics['False Negatives']= confusion_matrix(y_true,y_pred)[1][0]\n",
    "    #metrics['Confusion Matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "y_pred = predict(weights, X_test, y_test)\n",
    "metrics = evaluate_model(y_test, y_pred)\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953bf0f2",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c96821",
   "metadata": {},
   "source": [
    "## The logistic regression model shows a high level of accuracy (82.12%) in predicting the survival outcomes of passengers on the Titanic. The model also demonstrates a good balance between Precision and Recall, as evidenced by the F1 Score of 79.22%. This suggests that the model is both reliable and robust in identifying both classesâ€”those who survived and those who did not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cb707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
